{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compare models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\simon\\anaconda3\\envs\\acv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import random\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "from src.data.data_converter import tokens_to_weights, weights_to_flattened_weights, flattened_weights_to_weights\n",
    "from src.data.inr_dataset import INRDataset\n",
    "from src.data.utils import get_files_from_selectors\n",
    "from src.data.inr import INR\n",
    "\n",
    "from src.core.config import TransformerExperimentConfig, DataConfig, DataSelector, DatasetType\n",
    "from src.core.config_diffusion import DiffusionExperimentConfig\n",
    "\n",
    "from src.models.diffusion.pl_diffusion import HyperDiffusion\n",
    "from src.models.autoencoder.pl_transformer import Autoencoder\n",
    "\n",
    "from src.evaluation import model_utils, visualization_utils, metrics\n",
    "\n",
    "print(torch.cuda.is_available())\n",
    "\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cpu')"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "INR(\n",
       "  (seq): Sequential(\n",
       "    (0): Siren(\n",
       "      (activation): Sine()\n",
       "    )\n",
       "    (1): Siren(\n",
       "      (activation): Sine()\n",
       "    )\n",
       "    (2): Linear(in_features=32, out_features=1, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mlp = INR(up_scale=16)\n",
    "mlp.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "files = get_files_from_selectors(\"mnist-inrs\", [DataSelector(dataset_type=DatasetType.MNIST, class_label=2)])\n",
    "dataset_flattened = INRDataset(files, \"cpu\", is_flattened=True)\n",
    "dataset_tokenized = INRDataset(files, \"cpu\", is_flattened=False)\n",
    "ref_cp = dataset_flattened.get_state_dict(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Create Hyperdiffusion model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using following input parameter splits: [np.int64(64), np.int64(32), np.int64(1024), np.int64(32), np.int64(32), np.int64(1), 257]\n",
      "['seq.0.weight', 'seq.0.bias', 'seq.1.weight', 'seq.1.bias', 'seq.2.weight', 'seq.2.bias', 'timestep_embedding']\n",
      "number of parameters: 103,471,104\n",
      "Dataset size: 6990\n"
     ]
    }
   ],
   "source": [
    "config: DiffusionExperimentConfig = DiffusionExperimentConfig.sanity()\n",
    "\n",
    "config.transformer_config.n_embd = 1024\n",
    "config.transformer_config.n_head = 8\n",
    "config.transformer_config.n_layer = 8\n",
    "#config.data = DataConfig.small()\n",
    "#config.data.selector = \n",
    "#data_path = os.path.join(os.getcwd(), config.data.data_path)\n",
    "\n",
    "data_shape = dataset_flattened[0].unsqueeze(0).shape\n",
    "# Initialize model\n",
    "hyperdiffusion = HyperDiffusion(\n",
    "    config, data_shape\n",
    ")\n",
    "hyperdiffusion.eval()\n",
    "\n",
    "#checkpoint = \"good_checkpoints/ferdy/hyperdiffusion/standard_hyperdiffusion.ckpt\"\n",
    "checkpoint = \"good_checkpoints/ferdy/hyperdiffusion/hyperdiffusion_20250126_132035.ckpt\"\n",
    "state_dict = torch.load(checkpoint, map_location=torch.device('cpu'))[\"state_dict\"]\n",
    "hyperdiffusion.load_state_dict(state_dict)\n",
    "print(\"Dataset size:\", len(dataset_flattened))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Create VAE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "50473170"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config_ae: TransformerExperimentConfig = TransformerExperimentConfig.default()\n",
    "config_ae.model.num_heads = 8\n",
    "config_ae.model.num_layers = 8\n",
    "config_ae.model.d_model = 512  # 256 -> 4\n",
    "config_ae.model.latent_dim = 8\n",
    "config_ae.model.layer_norm = False\n",
    "config_ae.model.use_mask = True\n",
    "\n",
    "autoencoder_checkpoint = \"good_checkpoints/best_overfit_so_far_099_split.ckpt\"\n",
    "#autoencoder_checkpoint = \"good_checkpoints/overfit.ckpt\"\n",
    "# Initialize model\n",
    "vae = Autoencoder(config_ae)\n",
    "vae.eval()\n",
    "state_dict = torch.load(autoencoder_checkpoint, map_location=torch.device('cpu'))\n",
    "vae.load_state_dict(state_dict[\"state_dict\"])\n",
    "sum(p.numel() for p in vae.parameters())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Create Stable Hyperdiffusion Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using following input parameter splits: [520, 257]\n",
      "['layer0', 'timestep_embedding']\n",
      "number of parameters: 25,886,208\n",
      "\n"
     ]
    }
   ],
   "source": [
    "config_shyp: DiffusionExperimentConfig = DiffusionExperimentConfig.sanity()\n",
    "config_shyp.transformer_config.n_embd = 512\n",
    "config_shyp.transformer_config.n_head = 8\n",
    "config_shyp.transformer_config.n_layer = 8\n",
    "\n",
    "data_shape = (0, config_ae.model.n_tokens * config_ae.model.latent_dim)\n",
    "_,_,positions = dataset_tokenized[0]\n",
    "\n",
    "# Initialize model\n",
    "stable_hyperdiffusion = HyperDiffusion(\n",
    "    config_shyp, data_shape, vae, positions\n",
    ")\n",
    "\n",
    "diffusion_checkpoint = \"diffusion_logs/lightning_checkpoints/stable_hyperdiffusion_whole_dataset_2025-01-24 16-17-16.651982-hyperdiffusion_num2 20250124_161713-z8ljsssp/last.ckpt\"\n",
    "state_dict = torch.load(diffusion_checkpoint, map_location=torch.device('cpu'))\n",
    "stable_hyperdiffusion.load_state_dict(state_dict[\"state_dict\"])\n",
    "stable_hyperdiffusion.eval()\n",
    "print(\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Visualize the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_samples = 100\n",
    "mlp = mlp.to(device)\n",
    "hyperdiffusion = hyperdiffusion.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"evaluation/hyperdiffusion\"\n",
    "os.makedirs(path, exist_ok=True)\n",
    "for i in range(100):\n",
    "    samples = hyperdiffusion.generate_samples(n_samples)\n",
    "    #hyperdiffusion_images = model_utils.generate_diffusion_images(hyperdiffusion, mlp, n_samples, ref_cp=ref_cp)\n",
    "    #hyperdiffusion_images_numpy = np.array(hyperdiffusion_images)\n",
    "    print(samples.shape)\n",
    "    torch.save(samples, os.path.join(path, f\"hyperdiffusion_{i}.pt\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HyperDiffusion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([10000, 1185])\n"
     ]
    }
   ],
   "source": [
    "path = \"evaluation/hyperdiffusion/ferdy\"\n",
    "file_paths = [os.path.join(root, file) for root, _, files in os.walk(path) for file in files]\n",
    "tensors = [torch.load(f, map_location=\"cpu\") for f in file_paths]\n",
    "hyperdiffusion_vectors = torch.cat(tensors, dim=0)\n",
    "print(hyperdiffusion_vectors.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "hyperdiffusion_images = [model_utils.compute_image(mlp, flattened_weights_to_weights(weights, mlp)) for weights in hyperdiffusion_vectors]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_images_per_row = 5\n",
    "num_images = 200\n",
    "for i in range(min(num_images, len(hyperdiffusion_images))//num_images_per_row):\n",
    "    images = hyperdiffusion_images[i*num_images_per_row:i*num_images_per_row+num_images_per_row]\n",
    "    visualization_utils.plot_n_images(images, single_row=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([6990, 1185])\n"
     ]
    }
   ],
   "source": [
    "dataset_vectors = [dataset_flattened[i] for i in range(len(dataset_flattened))]\n",
    "dataset_vectors_array = torch.stack(dataset_vectors)\n",
    "print(dataset_vectors_array.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([6990, 784])\n"
     ]
    }
   ],
   "source": [
    "dataset_images = [model_utils.compute_image(mlp, flattened_weights_to_weights(dataset_flattened[i], mlp)) for i in range(len(dataset_flattened))]\n",
    "dataset_images_array = torch.Tensor(dataset_images)\n",
    "dataset_images_array = dataset_images_array.view(dataset_images_array.size(0), -1)\n",
    "print(dataset_images_array.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([10000, 28, 28])\n"
     ]
    }
   ],
   "source": [
    "hyperdiffusion_images_array =  torch.Tensor(hyperdiffusion_images)\n",
    "hyperdiffusion_images_array = hyperdiffusion_images_array.view(hyperdiffusion_images_array.size(0), -1)\n",
    "print(hyperdiffusion_images_array.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "f:\\University\\Master\\1.Semester\\Advanced DL for Computer Vision\\src\\evaluation\\metrics.py:50: UserWarning: The use of `x.T` on tensors of dimension other than 2 to reverse their shape is deprecated and it will throw an error in a future release. Consider `x.mT` to transpose batches of matrices or `x.permute(*torch.arange(x.ndim - 1, -1, -1))` to reverse the dimensions of a tensor. (Triggered internally at C:\\b\\abs_fakvb73nko\\croot\\pytorch-select_1730848725921\\work\\aten\\src\\ATen\\native\\TensorShape.cpp:3679.)\n",
      "  similarities = torch.matmul(diffusion_outputs_norm, dataset_samples_norm.T)\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "The size of tensor a (10000) must match the size of tensor b (28) at non-singleton dimension 0",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[22], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Get cosine similarity for closest images\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m indices, distances \u001b[38;5;241m=\u001b[39m \u001b[43mmetrics\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfind_nearest_neighbor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhyperdiffusion_images_array\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdataset_images_array\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCosine similarity for images: \u001b[39m\u001b[38;5;124m\"\u001b[39m, distances\u001b[38;5;241m.\u001b[39mmean()\u001b[38;5;241m.\u001b[39mitem())\n",
      "File \u001b[1;32mf:\\University\\Master\\1.Semester\\Advanced DL for Computer Vision\\src\\evaluation\\metrics.py:50\u001b[0m, in \u001b[0;36mfind_nearest_neighbor\u001b[1;34m(generated_samples, dataset_samples, metric, k)\u001b[0m\n\u001b[0;32m     48\u001b[0m     diffusion_outputs_norm \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mnormalize(generated_samples, p\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)  \u001b[38;5;66;03m# Shape: (7000, 65*33)\u001b[39;00m\n\u001b[0;32m     49\u001b[0m     dataset_samples_norm \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mnormalize(dataset_samples, p\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)  \u001b[38;5;66;03m# Shape: (num_samples, 65*33)\u001b[39;00m\n\u001b[1;32m---> 50\u001b[0m     similarities \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmatmul\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdiffusion_outputs_norm\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdataset_samples_norm\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mT\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     51\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m metric \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124meuclidean\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m     52\u001b[0m     similarities \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39msqrt(torch\u001b[38;5;241m.\u001b[39msum((generated_samples\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m1\u001b[39m) \u001b[38;5;241m-\u001b[39m dataset_samples\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m0\u001b[39m)) \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m \u001b[38;5;241m2\u001b[39m, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m))\n",
      "\u001b[1;31mRuntimeError\u001b[0m: The size of tensor a (10000) must match the size of tensor b (28) at non-singleton dimension 0"
     ]
    }
   ],
   "source": [
    "# Get cosine similarity for closest images\n",
    "indices, distances = metrics.find_nearest_neighbor(hyperdiffusion_images_array, dataset_images_array)\n",
    "print(\"Cosine similarity for images: \", distances.mean().item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Euclidean distance for images:  20.074223\n"
     ]
    }
   ],
   "source": [
    "# Get euclidean distance for closest images\n",
    "num_images = 100\n",
    "distances_total = []\n",
    "for i in range(len(hyperdiffusion_images_array)//num_images):\n",
    "    images = hyperdiffusion_images_array[i*num_images:i*num_images+num_images]\n",
    "    indices, distances = metrics.find_nearest_neighbor(images, dataset_images_array, metric=\"euclidean\")\n",
    "    distances_total.extend(distances)\n",
    "print(\"Euclidean distance for images: \", np.mean(distances_total))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Manhattan distance for images:  472.38705\n"
     ]
    }
   ],
   "source": [
    "# Get manhattan distance for closest images\n",
    "num_images = 100\n",
    "distances_total = []\n",
    "for i in range(len(hyperdiffusion_images_array)//num_images):\n",
    "    images = hyperdiffusion_images_array[i*num_images:i*num_images+num_images]\n",
    "    indices, distances = metrics.find_nearest_neighbor(images, dataset_images_array, metric=\"manhattan\")\n",
    "    distances_total.extend(distances)\n",
    "print(\"Manhattan distance for images: \", np.mean(distances_total))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cosine similarity for weights:  0.3276207447052002\n"
     ]
    }
   ],
   "source": [
    "# Get cosine similarity for closest weights\n",
    "indices, distances = metrics.find_nearest_neighbor(hyperdiffusion_vectors, dataset_vectors_array)\n",
    "print(\"Cosine similarity for weights: \", distances.mean().item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Euclidean distance for weights:  1.7096492\n"
     ]
    }
   ],
   "source": [
    "# Get cosine similarity for closest weights\n",
    "num_images = 100\n",
    "distances_total = []\n",
    "for i in range(len(hyperdiffusion_vectors)//num_images):\n",
    "    images = hyperdiffusion_vectors[i*num_images:i*num_images+num_images]\n",
    "    indices, distances = metrics.find_nearest_neighbor(images, dataset_vectors_array, metric=\"euclidean\")\n",
    "    distances_total.extend(distances)\n",
    "print(\"Euclidean distance for weights: \", np.mean(distances_total))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Manhattan distance for weights:  34.595654\n"
     ]
    }
   ],
   "source": [
    "# Get manhattan distance for closest weights\n",
    "num_images = 100\n",
    "distances_total = []\n",
    "for i in range(len(hyperdiffusion_vectors)//num_images):\n",
    "    images = hyperdiffusion_vectors[i*num_images:i*num_images+num_images]\n",
    "    indices, distances = metrics.find_nearest_neighbor(images, dataset_vectors_array, metric=\"manhattan\")\n",
    "    distances_total.extend(distances)\n",
    "print(\"Manhattan distance for weights: \", np.mean(distances_total))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stable HyperDiffusion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#path = \"evaluation/stable_hyperdiffusion\"\n",
    "#n_samples = 100\n",
    "#os.makedirs(path, exist_ok=True)\n",
    "#for i in range(100):\n",
    "#    samples_tokenized, samples_reconstructed, positions = stable_hyperdiffusion.generate_samples(n_samples)\n",
    "#    #hyperdiffusion_images = model_utils.generate_diffusion_images(hyperdiffusion, mlp, n_samples, ref_cp=ref_cp)\n",
    "#    #hyperdiffusion_images_numpy = np.array(hyperdiffusion_images)\n",
    "#    print(samples_tokenized.shape, samples_reconstructed.shape, positions.shape)\n",
    " #   torch.save((samples_tokenized, samples_reconstructed, positions), os.path.join(path, f\"stable_hyperdiffusion_{i}.pt\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['evaluation/stable_hyperdiffusion\\\\standard_hyperdiffusion_0.pt', 'evaluation/stable_hyperdiffusion\\\\standard_hyperdiffusion_1.pt', 'evaluation/stable_hyperdiffusion\\\\standard_hyperdiffusion_10.pt', 'evaluation/stable_hyperdiffusion\\\\standard_hyperdiffusion_11.pt', 'evaluation/stable_hyperdiffusion\\\\standard_hyperdiffusion_12.pt', 'evaluation/stable_hyperdiffusion\\\\standard_hyperdiffusion_13.pt', 'evaluation/stable_hyperdiffusion\\\\standard_hyperdiffusion_14.pt', 'evaluation/stable_hyperdiffusion\\\\standard_hyperdiffusion_15.pt', 'evaluation/stable_hyperdiffusion\\\\standard_hyperdiffusion_16.pt', 'evaluation/stable_hyperdiffusion\\\\standard_hyperdiffusion_17.pt', 'evaluation/stable_hyperdiffusion\\\\standard_hyperdiffusion_18.pt', 'evaluation/stable_hyperdiffusion\\\\standard_hyperdiffusion_19.pt', 'evaluation/stable_hyperdiffusion\\\\standard_hyperdiffusion_2.pt', 'evaluation/stable_hyperdiffusion\\\\standard_hyperdiffusion_20.pt', 'evaluation/stable_hyperdiffusion\\\\standard_hyperdiffusion_21.pt', 'evaluation/stable_hyperdiffusion\\\\standard_hyperdiffusion_22.pt', 'evaluation/stable_hyperdiffusion\\\\standard_hyperdiffusion_23.pt', 'evaluation/stable_hyperdiffusion\\\\standard_hyperdiffusion_24.pt', 'evaluation/stable_hyperdiffusion\\\\standard_hyperdiffusion_25.pt', 'evaluation/stable_hyperdiffusion\\\\standard_hyperdiffusion_26.pt', 'evaluation/stable_hyperdiffusion\\\\standard_hyperdiffusion_27.pt', 'evaluation/stable_hyperdiffusion\\\\standard_hyperdiffusion_28.pt', 'evaluation/stable_hyperdiffusion\\\\standard_hyperdiffusion_29.pt', 'evaluation/stable_hyperdiffusion\\\\standard_hyperdiffusion_3.pt', 'evaluation/stable_hyperdiffusion\\\\standard_hyperdiffusion_30.pt', 'evaluation/stable_hyperdiffusion\\\\standard_hyperdiffusion_31.pt', 'evaluation/stable_hyperdiffusion\\\\standard_hyperdiffusion_32.pt', 'evaluation/stable_hyperdiffusion\\\\standard_hyperdiffusion_33.pt', 'evaluation/stable_hyperdiffusion\\\\standard_hyperdiffusion_34.pt', 'evaluation/stable_hyperdiffusion\\\\standard_hyperdiffusion_35.pt', 'evaluation/stable_hyperdiffusion\\\\standard_hyperdiffusion_36.pt', 'evaluation/stable_hyperdiffusion\\\\standard_hyperdiffusion_37.pt', 'evaluation/stable_hyperdiffusion\\\\standard_hyperdiffusion_38.pt', 'evaluation/stable_hyperdiffusion\\\\standard_hyperdiffusion_39.pt', 'evaluation/stable_hyperdiffusion\\\\standard_hyperdiffusion_4.pt', 'evaluation/stable_hyperdiffusion\\\\standard_hyperdiffusion_40.pt', 'evaluation/stable_hyperdiffusion\\\\standard_hyperdiffusion_41.pt', 'evaluation/stable_hyperdiffusion\\\\standard_hyperdiffusion_42.pt', 'evaluation/stable_hyperdiffusion\\\\standard_hyperdiffusion_43.pt', 'evaluation/stable_hyperdiffusion\\\\standard_hyperdiffusion_44.pt', 'evaluation/stable_hyperdiffusion\\\\standard_hyperdiffusion_45.pt', 'evaluation/stable_hyperdiffusion\\\\standard_hyperdiffusion_46.pt', 'evaluation/stable_hyperdiffusion\\\\standard_hyperdiffusion_47.pt', 'evaluation/stable_hyperdiffusion\\\\standard_hyperdiffusion_48.pt', 'evaluation/stable_hyperdiffusion\\\\standard_hyperdiffusion_49.pt', 'evaluation/stable_hyperdiffusion\\\\standard_hyperdiffusion_5.pt', 'evaluation/stable_hyperdiffusion\\\\standard_hyperdiffusion_50.pt', 'evaluation/stable_hyperdiffusion\\\\standard_hyperdiffusion_51.pt', 'evaluation/stable_hyperdiffusion\\\\standard_hyperdiffusion_52.pt', 'evaluation/stable_hyperdiffusion\\\\standard_hyperdiffusion_53.pt', 'evaluation/stable_hyperdiffusion\\\\standard_hyperdiffusion_54.pt', 'evaluation/stable_hyperdiffusion\\\\standard_hyperdiffusion_55.pt', 'evaluation/stable_hyperdiffusion\\\\standard_hyperdiffusion_56.pt', 'evaluation/stable_hyperdiffusion\\\\standard_hyperdiffusion_57.pt', 'evaluation/stable_hyperdiffusion\\\\standard_hyperdiffusion_58.pt', 'evaluation/stable_hyperdiffusion\\\\standard_hyperdiffusion_59.pt', 'evaluation/stable_hyperdiffusion\\\\standard_hyperdiffusion_6.pt', 'evaluation/stable_hyperdiffusion\\\\standard_hyperdiffusion_60.pt', 'evaluation/stable_hyperdiffusion\\\\standard_hyperdiffusion_61.pt', 'evaluation/stable_hyperdiffusion\\\\standard_hyperdiffusion_62.pt', 'evaluation/stable_hyperdiffusion\\\\standard_hyperdiffusion_63.pt', 'evaluation/stable_hyperdiffusion\\\\standard_hyperdiffusion_64.pt', 'evaluation/stable_hyperdiffusion\\\\standard_hyperdiffusion_65.pt', 'evaluation/stable_hyperdiffusion\\\\standard_hyperdiffusion_66.pt', 'evaluation/stable_hyperdiffusion\\\\standard_hyperdiffusion_67.pt', 'evaluation/stable_hyperdiffusion\\\\standard_hyperdiffusion_68.pt', 'evaluation/stable_hyperdiffusion\\\\standard_hyperdiffusion_69.pt', 'evaluation/stable_hyperdiffusion\\\\standard_hyperdiffusion_7.pt', 'evaluation/stable_hyperdiffusion\\\\standard_hyperdiffusion_70.pt', 'evaluation/stable_hyperdiffusion\\\\standard_hyperdiffusion_71.pt', 'evaluation/stable_hyperdiffusion\\\\standard_hyperdiffusion_72.pt', 'evaluation/stable_hyperdiffusion\\\\standard_hyperdiffusion_73.pt', 'evaluation/stable_hyperdiffusion\\\\standard_hyperdiffusion_74.pt', 'evaluation/stable_hyperdiffusion\\\\standard_hyperdiffusion_75.pt', 'evaluation/stable_hyperdiffusion\\\\standard_hyperdiffusion_76.pt', 'evaluation/stable_hyperdiffusion\\\\standard_hyperdiffusion_77.pt', 'evaluation/stable_hyperdiffusion\\\\standard_hyperdiffusion_78.pt', 'evaluation/stable_hyperdiffusion\\\\standard_hyperdiffusion_79.pt', 'evaluation/stable_hyperdiffusion\\\\standard_hyperdiffusion_8.pt', 'evaluation/stable_hyperdiffusion\\\\standard_hyperdiffusion_80.pt', 'evaluation/stable_hyperdiffusion\\\\standard_hyperdiffusion_81.pt', 'evaluation/stable_hyperdiffusion\\\\standard_hyperdiffusion_82.pt', 'evaluation/stable_hyperdiffusion\\\\standard_hyperdiffusion_83.pt', 'evaluation/stable_hyperdiffusion\\\\standard_hyperdiffusion_84.pt', 'evaluation/stable_hyperdiffusion\\\\standard_hyperdiffusion_85.pt', 'evaluation/stable_hyperdiffusion\\\\standard_hyperdiffusion_86.pt', 'evaluation/stable_hyperdiffusion\\\\standard_hyperdiffusion_87.pt', 'evaluation/stable_hyperdiffusion\\\\standard_hyperdiffusion_88.pt', 'evaluation/stable_hyperdiffusion\\\\standard_hyperdiffusion_89.pt', 'evaluation/stable_hyperdiffusion\\\\standard_hyperdiffusion_9.pt', 'evaluation/stable_hyperdiffusion\\\\standard_hyperdiffusion_90.pt', 'evaluation/stable_hyperdiffusion\\\\standard_hyperdiffusion_91.pt', 'evaluation/stable_hyperdiffusion\\\\standard_hyperdiffusion_92.pt', 'evaluation/stable_hyperdiffusion\\\\standard_hyperdiffusion_93.pt', 'evaluation/stable_hyperdiffusion\\\\standard_hyperdiffusion_94.pt', 'evaluation/stable_hyperdiffusion\\\\standard_hyperdiffusion_95.pt', 'evaluation/stable_hyperdiffusion\\\\standard_hyperdiffusion_96.pt', 'evaluation/stable_hyperdiffusion\\\\standard_hyperdiffusion_97.pt', 'evaluation/stable_hyperdiffusion\\\\standard_hyperdiffusion_98.pt', 'evaluation/stable_hyperdiffusion\\\\standard_hyperdiffusion_99.pt']\n",
      "torch.Size([10000, 65, 8])\n",
      "torch.Size([10000, 65, 33])\n",
      "torch.Size([10000, 65, 3])\n"
     ]
    }
   ],
   "source": [
    "path = \"evaluation/stable_hyperdiffusion\"\n",
    "file_paths = [os.path.join(root, file) for root, _, files in os.walk(path) for file in files]\n",
    "print(file_paths)\n",
    "latens = []\n",
    "tokens = []\n",
    "pos = []\n",
    "for file in file_paths:\n",
    "    samples_tokenized, samples_reconstructed, positions = torch.load(file, map_location=\"cpu\")\n",
    "    latens.append(samples_tokenized)\n",
    "    tokens.append(samples_reconstructed)\n",
    "    pos.append(positions)\n",
    "stable_hyperdiffusion_latent = torch.cat(latens, dim=0)\n",
    "stable_hyperdiffusion_recon = torch.cat(tokens, dim=0)\n",
    "stable_hyperdiffusion_pos = torch.cat(pos, dim=0)\n",
    "print(stable_hyperdiffusion_latent.shape)\n",
    "print(stable_hyperdiffusion_recon.shape)\n",
    "print(stable_hyperdiffusion_pos.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "stable_hyperdiffusion_images = [model_utils.compute_image(mlp, tokens_to_weights(t, p, ref_cp)) for t, p in zip(stable_hyperdiffusion_recon, stable_hyperdiffusion_pos)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([10000, 28, 28])\n"
     ]
    }
   ],
   "source": [
    "stable_hyperdiffusion_images_array =  torch.Tensor(stable_hyperdiffusion_images)\n",
    "stable_hyperdiffusion_images_array = stable_hyperdiffusion_images_array.view(stable_hyperdiffusion_images_array.size(0), -1)\n",
    "print(stable_hyperdiffusion_images_array.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cosine similarity for images:  0.8701229691505432\n"
     ]
    }
   ],
   "source": [
    "# Get cosine similarity for closest images\n",
    "indices, distances = metrics.find_nearest_neighbor(stable_hyperdiffusion_images_array, dataset_images_array)\n",
    "print(\"Cosine similarity for images: \", distances.mean().item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Euclidean distance for images:  14.19277\n"
     ]
    }
   ],
   "source": [
    "# Get cosine similarity for closest images\n",
    "num_images = 100\n",
    "distances_total = []\n",
    "for i in range(len(stable_hyperdiffusion_images_array)//num_images):\n",
    "    images = stable_hyperdiffusion_images_array[i*num_images:i*num_images+num_images]\n",
    "    indices, distances = metrics.find_nearest_neighbor(images, dataset_images_array, metric=\"euclidean\")\n",
    "    distances_total.extend(distances)\n",
    "print(\"Euclidean distance for images: \", np.mean(distances_total))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Manhattan distance for images:  285.3252\n"
     ]
    }
   ],
   "source": [
    "# Get manhattan distance for closest images\n",
    "num_images = 100\n",
    "distances_total = []\n",
    "for i in range(len(stable_hyperdiffusion_images_array)//num_images):\n",
    "    images = stable_hyperdiffusion_images_array[i*num_images:i*num_images+num_images]\n",
    "    indices, distances = metrics.find_nearest_neighbor(images, dataset_images_array, metric=\"manhattan\")\n",
    "    distances_total.extend(distances)\n",
    "print(\"Manhattan distance for images: \", np.mean(distances_total))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([10000, 1185])\n"
     ]
    }
   ],
   "source": [
    "stable_hyperdiffusion_vectors = [weights_to_flattened_weights(tokens_to_weights(t, p, ref_cp)) for t, p in zip(stable_hyperdiffusion_recon, stable_hyperdiffusion_pos)]\n",
    "stable_hyperdiffusion_vectors = torch.stack(stable_hyperdiffusion_vectors)\n",
    "print(stable_hyperdiffusion_vectors.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cosine similarity for weights:  0.9959532022476196\n"
     ]
    }
   ],
   "source": [
    "# Get cosine similarity for closest weights\n",
    "indices, distances = metrics.find_nearest_neighbor(stable_hyperdiffusion_vectors, dataset_vectors_array)\n",
    "print(\"Cosine similarity for weights: \", distances.mean().item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Euclidean distance for weights:  1.7441115\n"
     ]
    }
   ],
   "source": [
    "# Get cosine similarity for closest weights\n",
    "num_images = 100\n",
    "distances_total = []\n",
    "for i in range(len(stable_hyperdiffusion_vectors)//num_images):\n",
    "    images = stable_hyperdiffusion_vectors[i*num_images:i*num_images+num_images]\n",
    "    indices, distances = metrics.find_nearest_neighbor(images, dataset_vectors_array, metric=\"euclidean\")\n",
    "    distances_total.extend(distances)\n",
    "print(\"Euclidean distance for weights: \", np.mean(distances_total))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Manhattan distance for weights:  35.279602\n"
     ]
    }
   ],
   "source": [
    "# Get manhattan distance for closest weights\n",
    "num_images = 100\n",
    "distances_total = []\n",
    "for i in range(len(stable_hyperdiffusion_vectors)//num_images):\n",
    "    images = stable_hyperdiffusion_vectors[i*num_images:i*num_images+num_images]\n",
    "    indices, distances = metrics.find_nearest_neighbor(images, dataset_vectors_array, metric=\"manhattan\")\n",
    "    distances_total.extend(distances)\n",
    "print(\"Manhattan distance for weights: \", np.mean(distances_total))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Calculate metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5.1 Image comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_samples = 200\n",
    "hyperdiffusion_results = hyperdiffusion.generate_samples(num_samples)\n",
    "stable_hyperdiffusion_latent_outputs, stable_hyperdiffusion_results, stable_hyperdiffusion_positions = stable_hyperdiffusion.generate_samples(num_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(hyperdiffusion_results.shape)\n",
    "print(stable_hyperdiffusion_results.shape)\n",
    "print(stable_hyperdiffusion_latent_outputs.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get n random samples from dataset\n",
    "indices = random.sample(range(len(dataset_flattened)), num_samples)\n",
    "dataset_samples = [dataset_flattened[i] for i in indices]\n",
    "dataset_samples = torch.stack(dataset_samples)\n",
    "print(dataset_samples.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stable_hyperdiffusion_results_flattened = [weights_to_flattened_weights(tokens_to_weights(t, p, ref_cp)) for t, p in zip(stable_hyperdiffusion_results, stable_hyperdiffusion_positions)]\n",
    "stable_hyperdiffusion_results_flattened = torch.stack(stable_hyperdiffusion_results_flattened)\n",
    "print(stable_hyperdiffusion_results_flattened.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate images and stack to tensor\n",
    "hyperdiffusion_images = [torch.from_numpy(model_utils.compute_image(mlp, flattened_weights_to_weights(s, mlp))) for s in hyperdiffusion_results]\n",
    "hyperdiffusion_images = torch.stack(hyperdiffusion_images)\n",
    "stable_hyperdiffusion_images = [torch.from_numpy(model_utils.compute_image(mlp, flattened_weights_to_weights(s, mlp))) for s in stable_hyperdiffusion_results_flattened]\n",
    "stable_hyperdiffusion_images = torch.stack(stable_hyperdiffusion_images)\n",
    "dataset_images = [torch.from_numpy(model_utils.compute_image(mlp, flattened_weights_to_weights(s, mlp))) for s in dataset_samples]\n",
    "dataset_images = torch.stack(dataset_images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_images_array = torch.Tensor(dataset_images)\n",
    "hyperdiffusion_images_array = torch.Tensor(hyperdiffusion_images)\n",
    "stable_hyperdiffusion_images_array = torch.Tensor(stable_hyperdiffusion_images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([6990, 28, 28])\n",
      "torch.Size([10000, 28, 28])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(None, None)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(dataset_images_array.shape), print(hyperdiffusion_images_array.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Calculate FID scores:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[31], line 20\u001b[0m\n\u001b[0;32m     18\u001b[0m     indices \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mrandom\u001b[38;5;241m.\u001b[39mchoice(\u001b[38;5;28mlen\u001b[39m(dataset_images_array), n_samples, replace\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m     19\u001b[0m     dataset_images_array_subset \u001b[38;5;241m=\u001b[39m dataset_images_array[indices]\n\u001b[1;32m---> 20\u001b[0m     fid_stable_hyperdiffusion \u001b[38;5;241m=\u001b[39m \u001b[43mmetrics\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcalculate_fid\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset_images_array_subset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstable_hyperdiffusion_images_array_subset\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     21\u001b[0m     fid_scores\u001b[38;5;241m.\u001b[39mappend(fid_stable_hyperdiffusion)\n\u001b[0;32m     22\u001b[0m fid_stable_hyperdiffusion \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mmean(fid_scores)\n",
      "File \u001b[1;32mf:\\University\\Master\\1.Semester\\Advanced DL for Computer Vision\\src\\evaluation\\metrics.py:40\u001b[0m, in \u001b[0;36mcalculate_fid\u001b[1;34m(real_images, generated_images)\u001b[0m\n\u001b[0;32m     37\u001b[0m real_images \u001b[38;5;241m=\u001b[39m einops\u001b[38;5;241m.\u001b[39mrepeat(real_images, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb h w -> b c h w\u001b[39m\u001b[38;5;124m\"\u001b[39m, c\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m3\u001b[39m)\u001b[38;5;241m.\u001b[39mto(torch\u001b[38;5;241m.\u001b[39muint8)\n\u001b[0;32m     38\u001b[0m generated_images \u001b[38;5;241m=\u001b[39m einops\u001b[38;5;241m.\u001b[39mrepeat(generated_images, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb h w -> b c h w\u001b[39m\u001b[38;5;124m\"\u001b[39m, c\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m3\u001b[39m)\u001b[38;5;241m.\u001b[39mto(torch\u001b[38;5;241m.\u001b[39muint8)\n\u001b[1;32m---> 40\u001b[0m \u001b[43mfid\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mupdate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mreal_images\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreal\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m     41\u001b[0m fid\u001b[38;5;241m.\u001b[39mupdate(generated_images, real\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m     42\u001b[0m fid_score \u001b[38;5;241m=\u001b[39m fid\u001b[38;5;241m.\u001b[39mcompute()\n",
      "File \u001b[1;32mc:\\Users\\simon\\anaconda3\\envs\\acv\\Lib\\site-packages\\torchmetrics\\metric.py:550\u001b[0m, in \u001b[0;36mMetric._wrap_update.<locals>.wrapped_func\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    548\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mset_grad_enabled(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_enable_grad):\n\u001b[0;32m    549\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 550\u001b[0m         \u001b[43mupdate\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    551\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[0;32m    552\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExpected all tensors to be on\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mstr\u001b[39m(err):\n",
      "File \u001b[1;32mc:\\Users\\simon\\anaconda3\\envs\\acv\\Lib\\site-packages\\torchmetrics\\image\\fid.py:369\u001b[0m, in \u001b[0;36mFrechetInceptionDistance.update\u001b[1;34m(self, imgs, real)\u001b[0m\n\u001b[0;32m    360\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Update the state with extracted features.\u001b[39;00m\n\u001b[0;32m    361\u001b[0m \n\u001b[0;32m    362\u001b[0m \u001b[38;5;124;03mArgs:\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    366\u001b[0m \n\u001b[0;32m    367\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    368\u001b[0m imgs \u001b[38;5;241m=\u001b[39m (imgs \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m255\u001b[39m)\u001b[38;5;241m.\u001b[39mbyte() \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnormalize \u001b[38;5;129;01mand\u001b[39;00m (\u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mused_custom_model) \u001b[38;5;28;01melse\u001b[39;00m imgs\n\u001b[1;32m--> 369\u001b[0m features \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minception\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimgs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    370\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39morig_dtype \u001b[38;5;241m=\u001b[39m features\u001b[38;5;241m.\u001b[39mdtype\n\u001b[0;32m    371\u001b[0m features \u001b[38;5;241m=\u001b[39m features\u001b[38;5;241m.\u001b[39mdouble()\n",
      "File \u001b[1;32mc:\\Users\\simon\\anaconda3\\envs\\acv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\simon\\anaconda3\\envs\\acv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\simon\\anaconda3\\envs\\acv\\Lib\\site-packages\\torchmetrics\\image\\fid.py:156\u001b[0m, in \u001b[0;36mNoTrainInceptionV3.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m    154\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m    155\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Forward pass of neural network with reshaping of output.\"\"\"\u001b[39;00m\n\u001b[1;32m--> 156\u001b[0m     out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_torch_fidelity_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    157\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m out[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mreshape(x\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m], \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\simon\\anaconda3\\envs\\acv\\Lib\\site-packages\\torchmetrics\\image\\fid.py:102\u001b[0m, in \u001b[0;36mNoTrainInceptionV3._torch_fidelity_forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     99\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(remaining_features) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m    100\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mtuple\u001b[39m(features[a] \u001b[38;5;28;01mfor\u001b[39;00m a \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfeatures_list)\n\u001b[1;32m--> 102\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mConv2d_3b_1x1\u001b[49m(x)\n\u001b[0;32m    103\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mConv2d_4a_3x3(x)\n\u001b[0;32m    104\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mMaxPool_2(x)\n",
      "File \u001b[1;32mc:\\Users\\simon\\anaconda3\\envs\\acv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1696\u001b[0m, in \u001b[0;36mModule.__getattr__\u001b[1;34m(self, name)\u001b[0m\n\u001b[0;32m   1687\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;241m=\u001b[39m OrderedDict()\n\u001b[0;32m   1689\u001b[0m \u001b[38;5;66;03m# On the return type:\u001b[39;00m\n\u001b[0;32m   1690\u001b[0m \u001b[38;5;66;03m# We choose to return `Any` in the `__getattr__` type signature instead of a more strict `Union[Tensor, Module]`.\u001b[39;00m\n\u001b[0;32m   1691\u001b[0m \u001b[38;5;66;03m# This is done for better interop with various type checkers for the end users.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1694\u001b[0m \u001b[38;5;66;03m# See full discussion on the problems with returning `Union` here\u001b[39;00m\n\u001b[0;32m   1695\u001b[0m \u001b[38;5;66;03m# https://github.com/microsoft/pyright/issues/4213\u001b[39;00m\n\u001b[1;32m-> 1696\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m__getattr__\u001b[39m(\u001b[38;5;28mself\u001b[39m, name: \u001b[38;5;28mstr\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[0;32m   1697\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m_parameters\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__dict__\u001b[39m:\n\u001b[0;32m   1698\u001b[0m         _parameters \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__dict__\u001b[39m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m_parameters\u001b[39m\u001b[38;5;124m'\u001b[39m]\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "n_samples = 500\n",
    "fid_scores = []\n",
    "for i in range(len(hyperdiffusion_images_array)//n_samples):\n",
    "    hyperdiffusion_images_array_subset = hyperdiffusion_images_array[i*n_samples:(i+1)*n_samples]\n",
    "\n",
    "    # Take random samples from the dataset for computational efficiency\n",
    "    indices = np.random.choice(len(dataset_images_array), n_samples, replace=False)\n",
    "    dataset_images_array_subset = dataset_images_array[indices]\n",
    "    fid_hyperdiffusion = metrics.calculate_fid(dataset_images_array_subset, hyperdiffusion_images_array_subset)\n",
    "    fid_scores.append(fid_hyperdiffusion)\n",
    "fid_hyperdiffusion = np.mean(fid_scores)\n",
    "\n",
    "fid_scores = []\n",
    "for i in range(len(stable_hyperdiffusion_images_array)//n_samples):\n",
    "    stable_hyperdiffusion_images_array_subset = stable_hyperdiffusion_images_array[i*n_samples:(i+1)*n_samples]\n",
    "\n",
    "    # Take random samples from the dataset for computational efficiency\n",
    "    indices = np.random.choice(len(dataset_images_array), n_samples, replace=False)\n",
    "    dataset_images_array_subset = dataset_images_array[indices]\n",
    "    fid_stable_hyperdiffusion = metrics.calculate_fid(dataset_images_array_subset, stable_hyperdiffusion_images_array_subset)\n",
    "    fid_scores.append(fid_stable_hyperdiffusion)\n",
    "fid_stable_hyperdiffusion = np.mean(fid_scores)\n",
    "print(f\"FID Hyperdiffusion: {fid_hyperdiffusion}\")\n",
    "print(f\"FID Stable Hyperdiffusion: {fid_stable_hyperdiffusion}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "stable_hyperdiffusion_images_array = stable_hyperdiffusion_images_array.view(stable_hyperdiffusion_images_array.size(0), -1)[:len(dataset_images_array)]\n",
    "hyperdiffusion_images_array = hyperdiffusion_images_array.view(hyperdiffusion_images_array.size(0), -1)[:len(dataset_images_array)]\n",
    "dataset_images_array = dataset_images_array.view(dataset_images_array.size(0), -1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Calculate MSE scores:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE Hyperdiffusion: 0.4186370074748993\n",
      "MSE Stable Hyperdiffusion: 0.15143154561519623\n"
     ]
    }
   ],
   "source": [
    "mse_hyperdiffusion = metrics.calculate_mse(dataset_images_array, hyperdiffusion_images_array)\n",
    "mse_stable_hyperdiffusion = metrics.calculate_mse(dataset_images_array, stable_hyperdiffusion_images_array)\n",
    "print(f\"MSE Hyperdiffusion: {mse_hyperdiffusion}\")\n",
    "print(f\"MSE Stable Hyperdiffusion: {mse_stable_hyperdiffusion}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Calculate the Minimum Matching Distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Minimum Matching Distance Hyperdiffusion: 0.01691521145403385\n",
      "Minimum Matching Distance Stable Hyperdiffusion: 0.021208513528108597\n"
     ]
    }
   ],
   "source": [
    "mmd_hyperdiffusion = metrics.compute_mmd(dataset_images_array.reshape(dataset_images_array.shape[0], -1), hyperdiffusion_images_array.reshape(hyperdiffusion_images_array.shape[0], -1))\n",
    "mmd_stable_hyperdiffusion = metrics.compute_mmd(dataset_images_array.reshape(dataset_images_array.shape[0], -1), stable_hyperdiffusion_images_array.reshape(stable_hyperdiffusion_images_array.shape[0], -1))\n",
    "print(f\"Minimum Matching Distance Hyperdiffusion: {mmd_hyperdiffusion}\")\n",
    "print(f\"Minimum Matching Distance Stable Hyperdiffusion: {mmd_stable_hyperdiffusion}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Calculate the coverage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Coverage Hyperdiffusion: 0.0580829756795422\n",
      "Coverage Stable Hyperdiffusion: 0.9958512160228898\n"
     ]
    }
   ],
   "source": [
    "coverage_hyperdiffusion = metrics.compute_coverage(dataset_images_array.reshape(dataset_images_array.shape[0], -1), hyperdiffusion_images_array.reshape(hyperdiffusion_images_array.shape[0], -1))\n",
    "coverage_stable_hyperdiffusion = metrics.compute_coverage(dataset_images_array.reshape(dataset_images_array.shape[0], -1), stable_hyperdiffusion_images_array.reshape(stable_hyperdiffusion_images_array.shape[0], -1))\n",
    "print(f\"Coverage Hyperdiffusion: {coverage_hyperdiffusion}\")\n",
    "print(f\"Coverage Stable Hyperdiffusion: {coverage_stable_hyperdiffusion}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Calculate the 1-Nearest-Neighbor Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1-Nearest-Neighbor Accuracy Hyperdiffusion: 0.9968526466380544\n",
      "1-Nearest-Neighbor Accuracy Stable Hyperdiffusion: 0.7286838340486409\n"
     ]
    }
   ],
   "source": [
    "nna_hyperdiffusion = metrics.compute_1nna(dataset_images_array.reshape(dataset_images_array.shape[0], -1), hyperdiffusion_images_array.reshape(hyperdiffusion_images_array.shape[0], -1))\n",
    "nna_stable_hyperdiffusion = metrics.compute_1nna(dataset_images_array.reshape(dataset_images_array.shape[0], -1), stable_hyperdiffusion_images_array.reshape(stable_hyperdiffusion_images_array.shape[0], -1))\n",
    "print(f\"1-Nearest-Neighbor Accuracy Hyperdiffusion: {nna_hyperdiffusion}\")\n",
    "print(f\"1-Nearest-Neighbor Accuracy Stable Hyperdiffusion: {nna_stable_hyperdiffusion}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Calculate the novelty"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Novelty Hyperdiffusion: 6966 of 6990\n",
      "Novelty Stable Hyperdiffusion: 91 of 6990\n"
     ]
    }
   ],
   "source": [
    "# NOTE: maybe we should use the whole training dataset here\n",
    "novelty_hyperdiffusion = metrics.detect_novelty(dataset_images_array.reshape(dataset_images_array.shape[0], -1), hyperdiffusion_images_array.reshape(hyperdiffusion_images_array.shape[0], -1))\n",
    "novelty_stable_hyperdiffusion = metrics.detect_novelty(dataset_images_array.reshape(dataset_images_array.shape[0], -1), stable_hyperdiffusion_images_array.reshape(stable_hyperdiffusion_images_array.shape[0], -1))\n",
    "print(f\"Novelty Hyperdiffusion: {novelty_hyperdiffusion[0].sum()} of {len(novelty_hyperdiffusion[0])}\")\n",
    "print(f\"Novelty Stable Hyperdiffusion: {novelty_stable_hyperdiffusion[0].sum()} of {len(novelty_stable_hyperdiffusion[0])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Novelty Hyperdiffusion: 6990\n",
      "Novelty Stable Hyperdiffusion: 6105\n"
     ]
    }
   ],
   "source": [
    "novelty_hyperdiffusion = metrics.novelty_svm(dataset_images_array.reshape(dataset_images_array.shape[0], -1), hyperdiffusion_images_array.reshape(hyperdiffusion_images_array.shape[0], -1))\n",
    "novelty_stable_hyperdiffusion = metrics.novelty_svm(dataset_images_array.reshape(dataset_images_array.shape[0], -1), stable_hyperdiffusion_images_array.reshape(stable_hyperdiffusion_images_array.shape[0], -1))\n",
    "print(f\"Novelty Hyperdiffusion: {(novelty_hyperdiffusion == -1).sum()}\")\n",
    "print(f\"Novelty Stable Hyperdiffusion: {(novelty_stable_hyperdiffusion == -1).sum()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Calculate inception score (IS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchmetrics.image.inception import InceptionScore\n",
    "import einops\n",
    "\n",
    "is_metric = InceptionScore(normalize=True)\n",
    "hyperdiffusion_images = einops.repeat(hyperdiffusion_images_array, \"b h w -> b c h w\", c=3).to(torch.uint8)\n",
    "scores = []\n",
    "for i in range(len(hyperdiffusion_images)//1000):\n",
    "    scores.append(is_metric(hyperdiffusion_images[i*1000:(i+1)*1000]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "IS Hyperdiffusion: [1.9331901  0.07715262]\n"
     ]
    }
   ],
   "source": [
    "print(f\"IS Hyperdiffusion: {np.mean(np.array(scores), axis=0)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "stable_hyperdiffusion_images = einops.repeat(stable_hyperdiffusion_images_array, \"b h w -> b c h w\", c=3).to(torch.uint8)\n",
    "scores = []\n",
    "for i in range(len(stable_hyperdiffusion_images) // 1000):\n",
    "    scores.append(is_metric(stable_hyperdiffusion_images[i*1000:(i+1)*1000]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "IS Stable Hyperdiffusion: [1.7749846  0.06352087]\n"
     ]
    }
   ],
   "source": [
    "print(f\"IS Stable Hyperdiffusion: {np.mean(np.array(scores), axis=0)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Peak Signal-to-Noise Ratio (PSNR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PSNR hyperdiffusion: 9.481261253356934\n",
      "PSNR stable hyperdiffusion: 13.910905838012695\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torchmetrics\n",
    "\n",
    "psnr = torchmetrics.PeakSignalNoiseRatio()\n",
    "score = psnr(hyperdiffusion_images_array, dataset_images_array)\n",
    "print(f\"PSNR hyperdiffusion: {score}\")\n",
    "score = psnr(stable_hyperdiffusion_images_array, dataset_images_array)\n",
    "print(f\"PSNR stable hyperdiffusion: {score}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Structural Similarity Index (SSIM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset images shape: torch.Size([6990, 28, 28])\n",
      "Hyperdiffusion images shape: torch.Size([6990, 28, 28])\n",
      "Stable hyperdiffusion images shape: torch.Size([6990, 28, 28])\n"
     ]
    }
   ],
   "source": [
    "dataset_images_array = torch.Tensor(dataset_images)\n",
    "hyperdiffusion_images_array = torch.Tensor(hyperdiffusion_images)[:len(dataset_images)]\n",
    "stable_hyperdiffusion_images_array = torch.Tensor(stable_hyperdiffusion_images)[:len(dataset_images)]\n",
    "print(f\"Dataset images shape: {dataset_images_array.shape}\")\n",
    "print(f\"Hyperdiffusion images shape: {hyperdiffusion_images_array.shape}\")\n",
    "print(f\"Stable hyperdiffusion images shape: {stable_hyperdiffusion_images_array.shape}\")\t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SSIM score hyperdiffusion: 0.003365378826856613\n",
      "SSIM score stable hyperdiffusion: 0.06459593772888184\n"
     ]
    }
   ],
   "source": [
    "import torchmetrics\n",
    "\n",
    "ssim = torchmetrics.StructuralSimilarityIndexMeasure(data_range=1.0)\n",
    "hyperdiffusion_images = einops.repeat(hyperdiffusion_images_array, \"b h w -> b c h w\", c=3)\n",
    "stable_hyperdiffusion_images = einops.repeat(stable_hyperdiffusion_images_array, \"b h w -> b c h w\", c=3)\n",
    "dataset_images = einops.repeat(dataset_images_array, \"b h w -> b c h w\", c=3)\n",
    "score = ssim(hyperdiffusion_images, dataset_images)\n",
    "print(f\"SSIM score hyperdiffusion: {score}\")\n",
    "score = ssim(stable_hyperdiffusion_images, dataset_images)\n",
    "print(f\"SSIM score stable hyperdiffusion: {score}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5.2 Weights comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. VAE Latent Space"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sample from latent space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_samples = 50\n",
    "latent_images = model_utils.sample_from_latent_space(\n",
    "    vae, mlp, ref_cp, dataset_tokenized[0][2], config_ae.model.n_tokens, config_ae.model.latent_dim, n_samples\n",
    ")\n",
    "visualization_utils.plot_n_images(latent_images)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Interpolate latent space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_interpolation_steps = 10\n",
    "\n",
    "num_interpolations = 100\n",
    "\n",
    "for i in range(num_interpolations):\n",
    "    idx = random.randint(0, len(dataset_tokenized) - 1)\n",
    "    idy = random.randint(0, len(dataset_tokenized) - 1)\n",
    "    latent_vectors = [dataset_tokenized[idx][0], dataset_tokenized[idy][0]]\n",
    "    latent_positions = [dataset_tokenized[idx][2], dataset_tokenized[idy][2]]\n",
    "    latent_vectors = torch.stack(latent_vectors)\n",
    "    latent_positions = torch.stack(latent_positions)\n",
    "\n",
    "    vae.eval()\n",
    "    with torch.no_grad():\n",
    "        latent_vector, _, _ = vae.encoder(latent_vectors, latent_positions)\n",
    "\n",
    "    latent_images = model_utils.interpolate_latent_space(\n",
    "        latent_vector[0], latent_vector[1], num_interpolation_steps, dataset_tokenized[0][2], vae, ref_cp, mlp\n",
    "    )\n",
    "    print(\"Indices:\", idx, idy)\n",
    "    visualization_utils.plot_n_images(latent_images, row=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "images, recons, mse_weights, mse_images = model_utils.get_n_images_and_mses(vae, dataset_tokenized, mlp, len(dataset_tokenized), device=device, random=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_mse_weight_indices = model_utils.get_best_samples(mse_weights, best_n=10)\n",
    "best_mse_image_indices = model_utils.get_best_samples(mse_images, best_n=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in best_mse_image_indices:\n",
    "    for j in best_mse_image_indices:\n",
    "        latent_vectors = [dataset_tokenized[i][0], dataset_tokenized[j][0]]\n",
    "        latent_positions = [dataset_tokenized[i][2], dataset_tokenized[j][2]]\n",
    "        latent_vectors = torch.stack(latent_vectors)\n",
    "        latent_positions = torch.stack(latent_positions)\n",
    "\n",
    "        vae.eval()\n",
    "        with torch.no_grad():\n",
    "            latent_vector, _, _ = vae.encoder(latent_vectors, latent_positions)\n",
    "\n",
    "        latent_images = model_utils.interpolate_latent_space(\n",
    "            latent_vector[0], latent_vector[1], num_interpolation_steps, dataset_tokenized[0][2], vae, ref_cp, mlp\n",
    "        )\n",
    "        visualization_utils.plot_n_images(latent_images, row=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7. Get nearest neighbors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_samples = 30\n",
    "k = 2\n",
    "dataset_images, hyperdiffusion_images, mse_images, mse_weights, distances = model_utils.generate_nearest_neighbors(hyperdiffusion, \n",
    "                                                                                                                  inr=mlp,\n",
    "                                                                                                                  dataset=dataset_flattened,\n",
    "                                                                                                                  num_samples=30,\n",
    "                                                                                                                  k=k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualization_utils.plot_diffusion_knn(\n",
    "    hyperdiffusion_images,\n",
    "    dataset_images,\n",
    "    mse_images,\n",
    "    mse_weights,\n",
    "    k=k,\n",
    "    num_samples=num_samples\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_images, stable_hyperdifusion_images, mse_images, mse_weights, distances = model_utils.generate_nearest_neighbors(stable_hyperdiffusion, \n",
    "                                                                                                                  inr=mlp,\n",
    "                                                                                                                  dataset=dataset_tokenized,\n",
    "                                                                                                                  num_samples=30,\n",
    "                                                                                                                  k=k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualization_utils.plot_diffusion_knn(\n",
    "    stable_hyperdifusion_images,\n",
    "    dataset_images,\n",
    "    mse_images,\n",
    "    mse_weights,\n",
    "    k=k,\n",
    "    num_samples=num_samples\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analse latent space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset tokens: torch.Size([6990, 65, 33])\n",
      "Dataset positions: torch.Size([6990, 65, 3])\n"
     ]
    }
   ],
   "source": [
    "dataset_tokens = [dataset_tokenized[i][0] for i in range(len(dataset_tokenized))]\n",
    "dataset_tokens = torch.stack(dataset_tokens)\n",
    "dataset_positions = [dataset_tokenized[i][2] for i in range(len(dataset_tokenized))]\n",
    "dataset_positions = torch.stack(dataset_positions)\n",
    "print(\"Dataset tokens:\", dataset_tokens.shape)\n",
    "print(\"Dataset positions:\", dataset_positions.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['evaluation/stable_hyperdiffusion\\\\standard_hyperdiffusion_0.pt', 'evaluation/stable_hyperdiffusion\\\\standard_hyperdiffusion_1.pt', 'evaluation/stable_hyperdiffusion\\\\standard_hyperdiffusion_10.pt', 'evaluation/stable_hyperdiffusion\\\\standard_hyperdiffusion_11.pt', 'evaluation/stable_hyperdiffusion\\\\standard_hyperdiffusion_12.pt', 'evaluation/stable_hyperdiffusion\\\\standard_hyperdiffusion_13.pt', 'evaluation/stable_hyperdiffusion\\\\standard_hyperdiffusion_14.pt', 'evaluation/stable_hyperdiffusion\\\\standard_hyperdiffusion_15.pt', 'evaluation/stable_hyperdiffusion\\\\standard_hyperdiffusion_16.pt', 'evaluation/stable_hyperdiffusion\\\\standard_hyperdiffusion_17.pt', 'evaluation/stable_hyperdiffusion\\\\standard_hyperdiffusion_18.pt', 'evaluation/stable_hyperdiffusion\\\\standard_hyperdiffusion_19.pt', 'evaluation/stable_hyperdiffusion\\\\standard_hyperdiffusion_2.pt', 'evaluation/stable_hyperdiffusion\\\\standard_hyperdiffusion_20.pt', 'evaluation/stable_hyperdiffusion\\\\standard_hyperdiffusion_21.pt', 'evaluation/stable_hyperdiffusion\\\\standard_hyperdiffusion_22.pt', 'evaluation/stable_hyperdiffusion\\\\standard_hyperdiffusion_23.pt', 'evaluation/stable_hyperdiffusion\\\\standard_hyperdiffusion_24.pt', 'evaluation/stable_hyperdiffusion\\\\standard_hyperdiffusion_25.pt', 'evaluation/stable_hyperdiffusion\\\\standard_hyperdiffusion_26.pt', 'evaluation/stable_hyperdiffusion\\\\standard_hyperdiffusion_27.pt', 'evaluation/stable_hyperdiffusion\\\\standard_hyperdiffusion_28.pt', 'evaluation/stable_hyperdiffusion\\\\standard_hyperdiffusion_29.pt', 'evaluation/stable_hyperdiffusion\\\\standard_hyperdiffusion_3.pt', 'evaluation/stable_hyperdiffusion\\\\standard_hyperdiffusion_30.pt', 'evaluation/stable_hyperdiffusion\\\\standard_hyperdiffusion_31.pt', 'evaluation/stable_hyperdiffusion\\\\standard_hyperdiffusion_32.pt', 'evaluation/stable_hyperdiffusion\\\\standard_hyperdiffusion_33.pt', 'evaluation/stable_hyperdiffusion\\\\standard_hyperdiffusion_34.pt', 'evaluation/stable_hyperdiffusion\\\\standard_hyperdiffusion_35.pt', 'evaluation/stable_hyperdiffusion\\\\standard_hyperdiffusion_36.pt', 'evaluation/stable_hyperdiffusion\\\\standard_hyperdiffusion_37.pt', 'evaluation/stable_hyperdiffusion\\\\standard_hyperdiffusion_38.pt', 'evaluation/stable_hyperdiffusion\\\\standard_hyperdiffusion_39.pt', 'evaluation/stable_hyperdiffusion\\\\standard_hyperdiffusion_4.pt', 'evaluation/stable_hyperdiffusion\\\\standard_hyperdiffusion_40.pt', 'evaluation/stable_hyperdiffusion\\\\standard_hyperdiffusion_41.pt', 'evaluation/stable_hyperdiffusion\\\\standard_hyperdiffusion_42.pt', 'evaluation/stable_hyperdiffusion\\\\standard_hyperdiffusion_43.pt', 'evaluation/stable_hyperdiffusion\\\\standard_hyperdiffusion_44.pt', 'evaluation/stable_hyperdiffusion\\\\standard_hyperdiffusion_45.pt', 'evaluation/stable_hyperdiffusion\\\\standard_hyperdiffusion_46.pt', 'evaluation/stable_hyperdiffusion\\\\standard_hyperdiffusion_47.pt', 'evaluation/stable_hyperdiffusion\\\\standard_hyperdiffusion_48.pt', 'evaluation/stable_hyperdiffusion\\\\standard_hyperdiffusion_49.pt', 'evaluation/stable_hyperdiffusion\\\\standard_hyperdiffusion_5.pt', 'evaluation/stable_hyperdiffusion\\\\standard_hyperdiffusion_50.pt', 'evaluation/stable_hyperdiffusion\\\\standard_hyperdiffusion_51.pt', 'evaluation/stable_hyperdiffusion\\\\standard_hyperdiffusion_52.pt', 'evaluation/stable_hyperdiffusion\\\\standard_hyperdiffusion_53.pt', 'evaluation/stable_hyperdiffusion\\\\standard_hyperdiffusion_54.pt', 'evaluation/stable_hyperdiffusion\\\\standard_hyperdiffusion_55.pt', 'evaluation/stable_hyperdiffusion\\\\standard_hyperdiffusion_56.pt', 'evaluation/stable_hyperdiffusion\\\\standard_hyperdiffusion_57.pt', 'evaluation/stable_hyperdiffusion\\\\standard_hyperdiffusion_58.pt', 'evaluation/stable_hyperdiffusion\\\\standard_hyperdiffusion_59.pt', 'evaluation/stable_hyperdiffusion\\\\standard_hyperdiffusion_6.pt', 'evaluation/stable_hyperdiffusion\\\\standard_hyperdiffusion_60.pt', 'evaluation/stable_hyperdiffusion\\\\standard_hyperdiffusion_61.pt', 'evaluation/stable_hyperdiffusion\\\\standard_hyperdiffusion_62.pt', 'evaluation/stable_hyperdiffusion\\\\standard_hyperdiffusion_63.pt', 'evaluation/stable_hyperdiffusion\\\\standard_hyperdiffusion_64.pt', 'evaluation/stable_hyperdiffusion\\\\standard_hyperdiffusion_65.pt', 'evaluation/stable_hyperdiffusion\\\\standard_hyperdiffusion_66.pt', 'evaluation/stable_hyperdiffusion\\\\standard_hyperdiffusion_67.pt', 'evaluation/stable_hyperdiffusion\\\\standard_hyperdiffusion_68.pt', 'evaluation/stable_hyperdiffusion\\\\standard_hyperdiffusion_69.pt', 'evaluation/stable_hyperdiffusion\\\\standard_hyperdiffusion_7.pt', 'evaluation/stable_hyperdiffusion\\\\standard_hyperdiffusion_70.pt', 'evaluation/stable_hyperdiffusion\\\\standard_hyperdiffusion_71.pt', 'evaluation/stable_hyperdiffusion\\\\standard_hyperdiffusion_72.pt', 'evaluation/stable_hyperdiffusion\\\\standard_hyperdiffusion_73.pt', 'evaluation/stable_hyperdiffusion\\\\standard_hyperdiffusion_74.pt', 'evaluation/stable_hyperdiffusion\\\\standard_hyperdiffusion_75.pt', 'evaluation/stable_hyperdiffusion\\\\standard_hyperdiffusion_76.pt', 'evaluation/stable_hyperdiffusion\\\\standard_hyperdiffusion_77.pt', 'evaluation/stable_hyperdiffusion\\\\standard_hyperdiffusion_78.pt', 'evaluation/stable_hyperdiffusion\\\\standard_hyperdiffusion_79.pt', 'evaluation/stable_hyperdiffusion\\\\standard_hyperdiffusion_8.pt', 'evaluation/stable_hyperdiffusion\\\\standard_hyperdiffusion_80.pt', 'evaluation/stable_hyperdiffusion\\\\standard_hyperdiffusion_81.pt', 'evaluation/stable_hyperdiffusion\\\\standard_hyperdiffusion_82.pt', 'evaluation/stable_hyperdiffusion\\\\standard_hyperdiffusion_83.pt', 'evaluation/stable_hyperdiffusion\\\\standard_hyperdiffusion_84.pt', 'evaluation/stable_hyperdiffusion\\\\standard_hyperdiffusion_85.pt', 'evaluation/stable_hyperdiffusion\\\\standard_hyperdiffusion_86.pt', 'evaluation/stable_hyperdiffusion\\\\standard_hyperdiffusion_87.pt', 'evaluation/stable_hyperdiffusion\\\\standard_hyperdiffusion_88.pt', 'evaluation/stable_hyperdiffusion\\\\standard_hyperdiffusion_89.pt', 'evaluation/stable_hyperdiffusion\\\\standard_hyperdiffusion_9.pt', 'evaluation/stable_hyperdiffusion\\\\standard_hyperdiffusion_90.pt', 'evaluation/stable_hyperdiffusion\\\\standard_hyperdiffusion_91.pt', 'evaluation/stable_hyperdiffusion\\\\standard_hyperdiffusion_92.pt', 'evaluation/stable_hyperdiffusion\\\\standard_hyperdiffusion_93.pt', 'evaluation/stable_hyperdiffusion\\\\standard_hyperdiffusion_94.pt', 'evaluation/stable_hyperdiffusion\\\\standard_hyperdiffusion_95.pt', 'evaluation/stable_hyperdiffusion\\\\standard_hyperdiffusion_96.pt', 'evaluation/stable_hyperdiffusion\\\\standard_hyperdiffusion_97.pt', 'evaluation/stable_hyperdiffusion\\\\standard_hyperdiffusion_98.pt', 'evaluation/stable_hyperdiffusion\\\\standard_hyperdiffusion_99.pt']\n",
      "torch.Size([10000, 65, 8])\n",
      "torch.Size([10000, 65, 33])\n",
      "torch.Size([10000, 65, 3])\n"
     ]
    }
   ],
   "source": [
    "path = \"evaluation/stable_hyperdiffusion\"\n",
    "file_paths = [os.path.join(root, file) for root, _, files in os.walk(path) for file in files]\n",
    "print(file_paths)\n",
    "latens = []\n",
    "tokens = []\n",
    "pos = []\n",
    "for file in file_paths:\n",
    "    samples_tokenized, samples_reconstructed, positions = torch.load(file, map_location=\"cpu\")\n",
    "    latens.append(samples_tokenized)\n",
    "    tokens.append(samples_reconstructed)\n",
    "    pos.append(positions)\n",
    "stable_hyperdiffusion_latent = torch.cat(latens, dim=0)\n",
    "stable_hyperdiffusion_recon = torch.cat(tokens, dim=0)\n",
    "stable_hyperdiffusion_pos = torch.cat(pos, dim=0)\n",
    "print(stable_hyperdiffusion_latent.shape)\n",
    "print(stable_hyperdiffusion_recon.shape)\n",
    "print(stable_hyperdiffusion_pos.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([6990, 65, 8])\n"
     ]
    }
   ],
   "source": [
    "vae.eval()\n",
    "with torch.no_grad():\n",
    "    dataset_latent,_,_ = vae.encoder(dataset_tokens, dataset_positions)\n",
    "print(dataset_latent.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([6990, 520])\n",
      "torch.Size([10000, 520])\n"
     ]
    }
   ],
   "source": [
    "dataset_latent_flattened = dataset_latent.flatten(start_dim =1)\n",
    "stable_hyperdiffusion_latent_flattened = stable_hyperdiffusion_latent.flatten(start_dim =1)\n",
    "print(dataset_latent_flattened.shape)\n",
    "print(stable_hyperdiffusion_latent_flattened.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cosine similarity for latent vectors:  0.9970109462738037\n"
     ]
    }
   ],
   "source": [
    "# Get cosine similarity for closest latent vectors\n",
    "indices, distances = metrics.find_nearest_neighbor(stable_hyperdiffusion_latent_flattened, dataset_latent_flattened)\n",
    "print(\"Cosine similarity for latent vectors: \", distances.mean().item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Euclidean distance for latent vectors:  6.470971\n"
     ]
    }
   ],
   "source": [
    "# Get cosine similarity for closest latent vectors\n",
    "num_images = 200\n",
    "distances_total = []\n",
    "for i in range(len(stable_hyperdiffusion_latent)//num_images):\n",
    "    latent_vectors = stable_hyperdiffusion_latent[i*num_images:i*num_images+num_images]\n",
    "    indices, distances = metrics.find_nearest_neighbor(latent_vectors, dataset_latent, metric=\"euclidean\")\n",
    "    distances_total.extend(distances)\n",
    "print(\"Euclidean distance for latent vectors: \", np.mean(distances_total))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Manhattan distance for latent vectors:  17.522793\n"
     ]
    }
   ],
   "source": [
    "# Get manhattan distance for closest latent vectors\n",
    "num_images = 200\n",
    "distances_total = []\n",
    "for i in range(len(stable_hyperdiffusion_latent)//num_images):\n",
    "    latent_vectors = stable_hyperdiffusion_latent[i*num_images:i*num_images+num_images]\n",
    "    indices, distances = metrics.find_nearest_neighbor(latent_vectors, dataset_latent, metric=\"manhattan\")\n",
    "    distances_total.extend(distances)\n",
    "print(\"Manhattan distance for latent vectors: \", np.mean(distances_total))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 8. Training and inference speed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cpu')"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stable_hyperdiffusion = stable_hyperdiffusion.to(device)\n",
    "hyperdiffusion = hyperdiffusion.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8.84 s ± 228 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%timeit stable_hyperdiffusion.generate_samples(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "35.3 s ± 277 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%timeit hyperdiffusion.generate_samples(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.data.inr_dataset import DataHandler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataConfig(data_path='mnist-inrs', selector=DataSelector(dataset_type=<DatasetType.MNIST: 'mnist'>, class_label=2, sample_id=None), batch_size=128, num_workers=4, sample_limit=None, split_ratio=0.9, load_from_txt=False)\n"
     ]
    }
   ],
   "source": [
    "config: DiffusionExperimentConfig = DiffusionExperimentConfig.sanity()\n",
    "config.data = DataConfig.small()\n",
    "config.data.batch_size = 128\n",
    "config.data.sample_limit = None\n",
    "print(config.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train size: 6291\n",
      "Val size: 699\n",
      "No overlapping files between train and val datasets.\n"
     ]
    }
   ],
   "source": [
    "# Setup dataloaders\n",
    "data_handler = DataHandler(config, use_autoencoder=False)\n",
    "data_handler.setup()\n",
    "hyperdiffusion_loader = data_handler.train_dataloader()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import get_linear_schedule_with_warmup\n",
    "\n",
    "def train(model, data_loader, epochs, device):\n",
    "    optimizer = torch.optim.AdamW(\n",
    "        model.parameters()\n",
    "    )\n",
    "\n",
    "    total_steps = len(data_loader) * epochs\n",
    "    warmup_steps = int(total_steps * 0.1)\n",
    "\n",
    "    # Linear warmup scheduler\n",
    "    scheduler = get_linear_schedule_with_warmup(\n",
    "        optimizer, num_warmup_steps=warmup_steps, num_training_steps=total_steps\n",
    "    )\n",
    "    model = model.to(device)\n",
    "    model.train()\n",
    "    for epoch in range(epochs):\n",
    "        for i, batch in enumerate(data_loader):\n",
    "            if model.autoencoder is None:\n",
    "                \n",
    "                batch = batch.to(device)\n",
    "                loss = model._compute_loss(batch)\n",
    "            else:\n",
    "                original_tokens, _, original_positions = batch\n",
    "                original_tokens = original_tokens.to(device)\n",
    "                original_positions = original_positions.to(device)\n",
    "\n",
    "                with torch.no_grad():\n",
    "                    latent_vector, _, _ = model.autoencoder.encoder(\n",
    "                        original_tokens, original_positions\n",
    "                    )  # Shape: (batch_size, n_tokens, latent_dim)\n",
    "\n",
    "                # Flatten all dimensions after batch (b) into a single dimension (-1 means auto-calculate size)\n",
    "                flattened_latent = einops.rearrange(latent_vector, \"b ... -> b (-1)\")\n",
    "\n",
    "                loss = model._compute_loss(flattened_latent)\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            scheduler.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%timeit train(hyperdiffusion, hyperdiffusion_loader, epochs=2, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup dataloaders\n",
    "data_handler = DataHandler(config, use_autoencoder=True)\n",
    "data_handler.setup()\n",
    "stable_hyperdiffusion_loader = data_handler.train_dataloader()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%timeit train(stable_hyperdiffusion, stable_hyperdiffusion_loader, epochs=2, device=device)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "acv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
