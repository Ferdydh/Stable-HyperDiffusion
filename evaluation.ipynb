{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compare models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "from src.data.data_converter import tokens_to_weights, weights_to_flattened_weights, flattened_weights_to_weights\n",
    "from src.data.inr_dataset import INRDataset\n",
    "from src.data.utils import get_files_from_selectors\n",
    "from src.data.inr import INR\n",
    "\n",
    "from src.core.config import TransformerExperimentConfig, DataConfig, DataSelector, DatasetType\n",
    "from src.core.config_diffusion import DiffusionExperimentConfig\n",
    "\n",
    "from src.models.diffusion.pl_diffusion import HyperDiffusion\n",
    "from src.models.autoencoder.pl_transformer import Autoencoder\n",
    "\n",
    "from src.evaluation import model_utils, visualization_utils, metrics\n",
    "\n",
    "print(torch.cuda.is_available())\n",
    "\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "vae_path = \"logs/best_overfit_so_far_099_split.ckpt\"\n",
    "standard_hyperdiffusion=\"diffusion_logs/lightning_checkpoints/2025-01-26 13-20-53.879333-standard hyperdiffusion, all 2 digits, big ass parameters, pls work 20250126_132035-962oirc5/last.ckpt\"\n",
    "stable_hyperdiffusion_path = \"logs/last (1).ckpt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlp = INR(up_scale=16)\n",
    "mlp.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "files = get_files_from_selectors(\"mnist-inrs\", [DataSelector(dataset_type=DatasetType.MNIST, class_label=2)])\n",
    "dataset_flattened = INRDataset(files, \"cpu\", is_flattened=True)\n",
    "dataset_tokenized = INRDataset(files, \"cpu\", is_flattened=False)\n",
    "ref_cp = dataset_flattened.get_state_dict(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Create Hyperdiffusion model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "config: DiffusionExperimentConfig = DiffusionExperimentConfig.sanity()\n",
    "\n",
    "config.transformer_config.n_embd = 1024\n",
    "config.transformer_config.n_head = 8\n",
    "config.transformer_config.n_layer = 8\n",
    "#config.data = DataConfig.small()\n",
    "#config.data.selector = \n",
    "#data_path = os.path.join(os.getcwd(), config.data.data_path)\n",
    "\n",
    "data_shape = dataset_flattened[0].unsqueeze(0).shape\n",
    "# Initialize model\n",
    "hyperdiffusion = HyperDiffusion(\n",
    "    config, data_shape\n",
    ")\n",
    "hyperdiffusion.eval()\n",
    "\n",
    "#checkpoint = \"good_checkpoints/ferdy/hyperdiffusion/standard_hyperdiffusion.ckpt\"\n",
    "#checkpoint = \"good_checkpoints/ferdy/hyperdiffusion/hyperdiffusion_20250126_132035.ckpt\"\n",
    "state_dict = torch.load(standard_hyperdiffusion, map_location=torch.device('cpu'))[\"state_dict\"]\n",
    "hyperdiffusion.load_state_dict(state_dict)\n",
    "print(\"Dataset size:\", len(dataset_flattened))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Create VAE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "config_ae: TransformerExperimentConfig = TransformerExperimentConfig.default()\n",
    "config_ae.model.num_heads = 8\n",
    "config_ae.model.num_layers = 8\n",
    "config_ae.model.d_model = 512  # 256 -> 4\n",
    "config_ae.model.latent_dim = 8\n",
    "config_ae.model.layer_norm = False\n",
    "config_ae.model.use_mask = True\n",
    "\n",
    "autoencoder_checkpoint = \"good_checkpoints/best_overfit_so_far_099_split.ckpt\"\n",
    "autoencoder_checkpoint = vae_path\n",
    "#autoencoder_checkpoint = \"good_checkpoints/overfit.ckpt\"\n",
    "# Initialize model\n",
    "vae = Autoencoder(config_ae)\n",
    "vae.eval()\n",
    "state_dict = torch.load(autoencoder_checkpoint, map_location=torch.device('cpu'))\n",
    "vae.load_state_dict(state_dict[\"state_dict\"])\n",
    "sum(p.numel() for p in vae.parameters())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Create Stable Hyperdiffusion Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "config_shyp: DiffusionExperimentConfig = DiffusionExperimentConfig.sanity()\n",
    "config_shyp.transformer_config.n_embd = 512\n",
    "config_shyp.transformer_config.n_head = 8\n",
    "config_shyp.transformer_config.n_layer = 8\n",
    "\n",
    "data_shape = (0, config_ae.model.n_tokens * config_ae.model.latent_dim)\n",
    "_,_,positions = dataset_tokenized[0]\n",
    "\n",
    "# Initialize model\n",
    "stable_hyperdiffusion = HyperDiffusion(\n",
    "    config_shyp, data_shape, vae, positions\n",
    ")\n",
    "\n",
    "diffusion_checkpoint = \"diffusion_logs/lightning_checkpoints/stable_hyperdiffusion_whole_dataset_2025-01-24 16-17-16.651982-hyperdiffusion_num2 20250124_161713-z8ljsssp/last.ckpt\"\n",
    "diffusion_checkpoint = stable_hyperdiffusion_path\n",
    "state_dict = torch.load(diffusion_checkpoint, map_location=torch.device('cpu'))\n",
    "stable_hyperdiffusion.load_state_dict(state_dict[\"state_dict\"])\n",
    "stable_hyperdiffusion.eval()\n",
    "print(\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Visualize the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_samples = 100\n",
    "mlp = mlp.to(device)\n",
    "hyperdiffusion = hyperdiffusion.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"evaluation/hyperdiffusion\"\n",
    "os.makedirs(path, exist_ok=True)\n",
    "for i in range(100):\n",
    "    samples = hyperdiffusion.generate_samples(n_samples)\n",
    "    #hyperdiffusion_images = model_utils.generate_diffusion_images(hyperdiffusion, mlp, n_samples, ref_cp=ref_cp)\n",
    "    #hyperdiffusion_images_numpy = np.array(hyperdiffusion_images)\n",
    "    print(samples.shape)\n",
    "    torch.save(samples, os.path.join(path, f\"hyperdiffusion_{i}.pt\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HyperDiffusion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"evaluation/hyperdiffusion/ferdy\"\n",
    "file_paths = [os.path.join(root, file) for root, _, files in os.walk(path) for file in files]\n",
    "tensors = [torch.load(f, map_location=\"cpu\") for f in file_paths]\n",
    "hyperdiffusion_vectors = torch.cat(tensors, dim=0)\n",
    "print(hyperdiffusion_vectors.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "hyperdiffusion_images = [model_utils.compute_image(mlp, flattened_weights_to_weights(weights, mlp)) for weights in hyperdiffusion_vectors]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_images_per_row = 5\n",
    "num_images = 200\n",
    "for i in range(min(num_images, len(hyperdiffusion_images))//num_images_per_row):\n",
    "    images = hyperdiffusion_images[i*num_images_per_row:i*num_images_per_row+num_images_per_row]\n",
    "    visualization_utils.plot_n_images(images, single_row=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_vectors = [dataset_flattened[i] for i in range(len(dataset_flattened))]\n",
    "dataset_vectors_array = torch.stack(dataset_vectors)\n",
    "print(dataset_vectors_array.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_images = [model_utils.compute_image(mlp, flattened_weights_to_weights(dataset_flattened[i], mlp)) for i in range(len(dataset_flattened))]\n",
    "dataset_images_array = torch.Tensor(dataset_images)\n",
    "dataset_images_array = dataset_images_array.view(dataset_images_array.size(0), -1)\n",
    "print(dataset_images_array.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "hyperdiffusion_images_array =  torch.Tensor(hyperdiffusion_images)\n",
    "hyperdiffusion_images_array = hyperdiffusion_images_array.view(hyperdiffusion_images_array.size(0), -1)\n",
    "print(hyperdiffusion_images_array.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get cosine similarity for closest images\n",
    "indices, distances = metrics.find_nearest_neighbor(hyperdiffusion_images_array, dataset_images_array)\n",
    "print(\"Cosine similarity for images: \", distances.mean().item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get euclidean distance for closest images\n",
    "num_images = 100\n",
    "distances_total = []\n",
    "for i in range(len(hyperdiffusion_images_array)//num_images):\n",
    "    images = hyperdiffusion_images_array[i*num_images:i*num_images+num_images]\n",
    "    indices, distances = metrics.find_nearest_neighbor(images, dataset_images_array, metric=\"euclidean\")\n",
    "    distances_total.extend(distances)\n",
    "print(\"Euclidean distance for images: \", np.mean(distances_total))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get manhattan distance for closest images\n",
    "num_images = 100\n",
    "distances_total = []\n",
    "for i in range(len(hyperdiffusion_images_array)//num_images):\n",
    "    images = hyperdiffusion_images_array[i*num_images:i*num_images+num_images]\n",
    "    indices, distances = metrics.find_nearest_neighbor(images, dataset_images_array, metric=\"manhattan\")\n",
    "    distances_total.extend(distances)\n",
    "print(\"Manhattan distance for images: \", np.mean(distances_total))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get cosine similarity for closest weights\n",
    "indices, distances = metrics.find_nearest_neighbor(hyperdiffusion_vectors, dataset_vectors_array)\n",
    "print(\"Cosine similarity for weights: \", distances.mean().item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get cosine similarity for closest weights\n",
    "num_images = 100\n",
    "distances_total = []\n",
    "for i in range(len(hyperdiffusion_vectors)//num_images):\n",
    "    images = hyperdiffusion_vectors[i*num_images:i*num_images+num_images]\n",
    "    indices, distances = metrics.find_nearest_neighbor(images, dataset_vectors_array, metric=\"euclidean\")\n",
    "    distances_total.extend(distances)\n",
    "print(\"Euclidean distance for weights: \", np.mean(distances_total))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get manhattan distance for closest weights\n",
    "num_images = 100\n",
    "distances_total = []\n",
    "for i in range(len(hyperdiffusion_vectors)//num_images):\n",
    "    images = hyperdiffusion_vectors[i*num_images:i*num_images+num_images]\n",
    "    indices, distances = metrics.find_nearest_neighbor(images, dataset_vectors_array, metric=\"manhattan\")\n",
    "    distances_total.extend(distances)\n",
    "print(\"Manhattan distance for weights: \", np.mean(distances_total))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stable HyperDiffusion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#path = \"evaluation/stable_hyperdiffusion\"\n",
    "#n_samples = 100\n",
    "#os.makedirs(path, exist_ok=True)\n",
    "#for i in range(100):\n",
    "#    samples_tokenized, samples_reconstructed, positions = stable_hyperdiffusion.generate_samples(n_samples)\n",
    "#    #hyperdiffusion_images = model_utils.generate_diffusion_images(hyperdiffusion, mlp, n_samples, ref_cp=ref_cp)\n",
    "#    #hyperdiffusion_images_numpy = np.array(hyperdiffusion_images)\n",
    "#    print(samples_tokenized.shape, samples_reconstructed.shape, positions.shape)\n",
    " #   torch.save((samples_tokenized, samples_reconstructed, positions), os.path.join(path, f\"stable_hyperdiffusion_{i}.pt\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"evaluation/stable_hyperdiffusion\"\n",
    "file_paths = [os.path.join(root, file) for root, _, files in os.walk(path) for file in files]\n",
    "print(file_paths)\n",
    "latens = []\n",
    "tokens = []\n",
    "pos = []\n",
    "for file in file_paths:\n",
    "    samples_tokenized, samples_reconstructed, positions = torch.load(file, map_location=\"cpu\")\n",
    "    latens.append(samples_tokenized)\n",
    "    tokens.append(samples_reconstructed)\n",
    "    pos.append(positions)\n",
    "stable_hyperdiffusion_latent = torch.cat(latens, dim=0)\n",
    "stable_hyperdiffusion_recon = torch.cat(tokens, dim=0)\n",
    "stable_hyperdiffusion_pos = torch.cat(pos, dim=0)\n",
    "print(stable_hyperdiffusion_latent.shape)\n",
    "print(stable_hyperdiffusion_recon.shape)\n",
    "print(stable_hyperdiffusion_pos.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "stable_hyperdiffusion_images = [model_utils.compute_image(mlp, tokens_to_weights(t, p, ref_cp)) for t, p in zip(stable_hyperdiffusion_recon, stable_hyperdiffusion_pos)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "stable_hyperdiffusion_images_array =  torch.Tensor(stable_hyperdiffusion_images)\n",
    "stable_hyperdiffusion_images_array = stable_hyperdiffusion_images_array.view(stable_hyperdiffusion_images_array.size(0), -1)\n",
    "print(stable_hyperdiffusion_images_array.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get cosine similarity for closest images\n",
    "indices, distances = metrics.find_nearest_neighbor(stable_hyperdiffusion_images_array, dataset_images_array)\n",
    "print(\"Cosine similarity for images: \", distances.mean().item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get cosine similarity for closest images\n",
    "num_images = 100\n",
    "distances_total = []\n",
    "for i in range(len(stable_hyperdiffusion_images_array)//num_images):\n",
    "    images = stable_hyperdiffusion_images_array[i*num_images:i*num_images+num_images]\n",
    "    indices, distances = metrics.find_nearest_neighbor(images, dataset_images_array, metric=\"euclidean\")\n",
    "    distances_total.extend(distances)\n",
    "print(\"Euclidean distance for images: \", np.mean(distances_total))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get manhattan distance for closest images\n",
    "num_images = 100\n",
    "distances_total = []\n",
    "for i in range(len(stable_hyperdiffusion_images_array)//num_images):\n",
    "    images = stable_hyperdiffusion_images_array[i*num_images:i*num_images+num_images]\n",
    "    indices, distances = metrics.find_nearest_neighbor(images, dataset_images_array, metric=\"manhattan\")\n",
    "    distances_total.extend(distances)\n",
    "print(\"Manhattan distance for images: \", np.mean(distances_total))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "stable_hyperdiffusion_vectors = [weights_to_flattened_weights(tokens_to_weights(t, p, ref_cp)) for t, p in zip(stable_hyperdiffusion_recon, stable_hyperdiffusion_pos)]\n",
    "stable_hyperdiffusion_vectors = torch.stack(stable_hyperdiffusion_vectors)\n",
    "print(stable_hyperdiffusion_vectors.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get cosine similarity for closest weights\n",
    "indices, distances = metrics.find_nearest_neighbor(stable_hyperdiffusion_vectors, dataset_vectors_array)\n",
    "print(\"Cosine similarity for weights: \", distances.mean().item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get cosine similarity for closest weights\n",
    "num_images = 100\n",
    "distances_total = []\n",
    "for i in range(len(stable_hyperdiffusion_vectors)//num_images):\n",
    "    images = stable_hyperdiffusion_vectors[i*num_images:i*num_images+num_images]\n",
    "    indices, distances = metrics.find_nearest_neighbor(images, dataset_vectors_array, metric=\"euclidean\")\n",
    "    distances_total.extend(distances)\n",
    "print(\"Euclidean distance for weights: \", np.mean(distances_total))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get manhattan distance for closest weights\n",
    "num_images = 100\n",
    "distances_total = []\n",
    "for i in range(len(stable_hyperdiffusion_vectors)//num_images):\n",
    "    images = stable_hyperdiffusion_vectors[i*num_images:i*num_images+num_images]\n",
    "    indices, distances = metrics.find_nearest_neighbor(images, dataset_vectors_array, metric=\"manhattan\")\n",
    "    distances_total.extend(distances)\n",
    "print(\"Manhattan distance for weights: \", np.mean(distances_total))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Calculate metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5.1 Image comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_samples = 200\n",
    "hyperdiffusion_results = hyperdiffusion.generate_samples(num_samples)\n",
    "stable_hyperdiffusion_latent_outputs, stable_hyperdiffusion_results, stable_hyperdiffusion_positions = stable_hyperdiffusion.generate_samples(num_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(hyperdiffusion_results.shape)\n",
    "print(stable_hyperdiffusion_results.shape)\n",
    "print(stable_hyperdiffusion_latent_outputs.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get n random samples from dataset\n",
    "indices = random.sample(range(len(dataset_flattened)), num_samples)\n",
    "dataset_samples = [dataset_flattened[i] for i in indices]\n",
    "dataset_samples = torch.stack(dataset_samples)\n",
    "print(dataset_samples.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stable_hyperdiffusion_results_flattened = [weights_to_flattened_weights(tokens_to_weights(t, p, ref_cp)) for t, p in zip(stable_hyperdiffusion_results, stable_hyperdiffusion_positions)]\n",
    "stable_hyperdiffusion_results_flattened = torch.stack(stable_hyperdiffusion_results_flattened)\n",
    "print(stable_hyperdiffusion_results_flattened.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate images and stack to tensor\n",
    "hyperdiffusion_images = [torch.from_numpy(model_utils.compute_image(mlp, flattened_weights_to_weights(s, mlp))) for s in hyperdiffusion_results]\n",
    "hyperdiffusion_images = torch.stack(hyperdiffusion_images)\n",
    "stable_hyperdiffusion_images = [torch.from_numpy(model_utils.compute_image(mlp, flattened_weights_to_weights(s, mlp))) for s in stable_hyperdiffusion_results_flattened]\n",
    "stable_hyperdiffusion_images = torch.stack(stable_hyperdiffusion_images)\n",
    "dataset_images = [torch.from_numpy(model_utils.compute_image(mlp, flattened_weights_to_weights(s, mlp))) for s in dataset_samples]\n",
    "dataset_images = torch.stack(dataset_images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_images_array = torch.Tensor(dataset_images)\n",
    "hyperdiffusion_images_array = torch.Tensor(hyperdiffusion_images)\n",
    "stable_hyperdiffusion_images_array = torch.Tensor(stable_hyperdiffusion_images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(dataset_images_array.shape), print(hyperdiffusion_images_array.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Calculate FID scores:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_samples = 500\n",
    "fid_scores = []\n",
    "for i in range(len(hyperdiffusion_images_array)//n_samples):\n",
    "    hyperdiffusion_images_array_subset = hyperdiffusion_images_array[i*n_samples:(i+1)*n_samples]\n",
    "\n",
    "    # Take random samples from the dataset for computational efficiency\n",
    "    indices = np.random.choice(len(dataset_images_array), n_samples, replace=False)\n",
    "    dataset_images_array_subset = dataset_images_array[indices]\n",
    "    fid_hyperdiffusion = metrics.calculate_fid(dataset_images_array_subset, hyperdiffusion_images_array_subset)\n",
    "    fid_scores.append(fid_hyperdiffusion)\n",
    "fid_hyperdiffusion = np.mean(fid_scores)\n",
    "\n",
    "fid_scores = []\n",
    "for i in range(len(stable_hyperdiffusion_images_array)//n_samples):\n",
    "    stable_hyperdiffusion_images_array_subset = stable_hyperdiffusion_images_array[i*n_samples:(i+1)*n_samples]\n",
    "\n",
    "    # Take random samples from the dataset for computational efficiency\n",
    "    indices = np.random.choice(len(dataset_images_array), n_samples, replace=False)\n",
    "    dataset_images_array_subset = dataset_images_array[indices]\n",
    "    fid_stable_hyperdiffusion = metrics.calculate_fid(dataset_images_array_subset, stable_hyperdiffusion_images_array_subset)\n",
    "    fid_scores.append(fid_stable_hyperdiffusion)\n",
    "fid_stable_hyperdiffusion = np.mean(fid_scores)\n",
    "print(f\"FID Hyperdiffusion: {fid_hyperdiffusion}\")\n",
    "print(f\"FID Stable Hyperdiffusion: {fid_stable_hyperdiffusion}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "stable_hyperdiffusion_images_array = stable_hyperdiffusion_images_array.view(stable_hyperdiffusion_images_array.size(0), -1)[:len(dataset_images_array)]\n",
    "hyperdiffusion_images_array = hyperdiffusion_images_array.view(hyperdiffusion_images_array.size(0), -1)[:len(dataset_images_array)]\n",
    "dataset_images_array = dataset_images_array.view(dataset_images_array.size(0), -1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Calculate MSE scores:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "mse_hyperdiffusion = metrics.calculate_mse(dataset_images_array, hyperdiffusion_images_array)\n",
    "mse_stable_hyperdiffusion = metrics.calculate_mse(dataset_images_array, stable_hyperdiffusion_images_array)\n",
    "print(f\"MSE Hyperdiffusion: {mse_hyperdiffusion}\")\n",
    "print(f\"MSE Stable Hyperdiffusion: {mse_stable_hyperdiffusion}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Calculate the Minimum Matching Distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "mmd_hyperdiffusion = metrics.compute_mmd(dataset_images_array.reshape(dataset_images_array.shape[0], -1), hyperdiffusion_images_array.reshape(hyperdiffusion_images_array.shape[0], -1))\n",
    "mmd_stable_hyperdiffusion = metrics.compute_mmd(dataset_images_array.reshape(dataset_images_array.shape[0], -1), stable_hyperdiffusion_images_array.reshape(stable_hyperdiffusion_images_array.shape[0], -1))\n",
    "print(f\"Minimum Matching Distance Hyperdiffusion: {mmd_hyperdiffusion}\")\n",
    "print(f\"Minimum Matching Distance Stable Hyperdiffusion: {mmd_stable_hyperdiffusion}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Calculate the coverage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "coverage_hyperdiffusion = metrics.compute_coverage(dataset_images_array.reshape(dataset_images_array.shape[0], -1), hyperdiffusion_images_array.reshape(hyperdiffusion_images_array.shape[0], -1))\n",
    "coverage_stable_hyperdiffusion = metrics.compute_coverage(dataset_images_array.reshape(dataset_images_array.shape[0], -1), stable_hyperdiffusion_images_array.reshape(stable_hyperdiffusion_images_array.shape[0], -1))\n",
    "print(f\"Coverage Hyperdiffusion: {coverage_hyperdiffusion}\")\n",
    "print(f\"Coverage Stable Hyperdiffusion: {coverage_stable_hyperdiffusion}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Calculate the 1-Nearest-Neighbor Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "nna_hyperdiffusion = metrics.compute_1nna(dataset_images_array.reshape(dataset_images_array.shape[0], -1), hyperdiffusion_images_array.reshape(hyperdiffusion_images_array.shape[0], -1))\n",
    "nna_stable_hyperdiffusion = metrics.compute_1nna(dataset_images_array.reshape(dataset_images_array.shape[0], -1), stable_hyperdiffusion_images_array.reshape(stable_hyperdiffusion_images_array.shape[0], -1))\n",
    "print(f\"1-Nearest-Neighbor Accuracy Hyperdiffusion: {nna_hyperdiffusion}\")\n",
    "print(f\"1-Nearest-Neighbor Accuracy Stable Hyperdiffusion: {nna_stable_hyperdiffusion}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Calculate the novelty"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOTE: maybe we should use the whole training dataset here\n",
    "novelty_hyperdiffusion = metrics.detect_novelty(dataset_images_array.reshape(dataset_images_array.shape[0], -1), hyperdiffusion_images_array.reshape(hyperdiffusion_images_array.shape[0], -1))\n",
    "novelty_stable_hyperdiffusion = metrics.detect_novelty(dataset_images_array.reshape(dataset_images_array.shape[0], -1), stable_hyperdiffusion_images_array.reshape(stable_hyperdiffusion_images_array.shape[0], -1))\n",
    "print(f\"Novelty Hyperdiffusion: {novelty_hyperdiffusion[0].sum()} of {len(novelty_hyperdiffusion[0])}\")\n",
    "print(f\"Novelty Stable Hyperdiffusion: {novelty_stable_hyperdiffusion[0].sum()} of {len(novelty_stable_hyperdiffusion[0])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "novelty_hyperdiffusion = metrics.novelty_svm(dataset_images_array.reshape(dataset_images_array.shape[0], -1), hyperdiffusion_images_array.reshape(hyperdiffusion_images_array.shape[0], -1))\n",
    "novelty_stable_hyperdiffusion = metrics.novelty_svm(dataset_images_array.reshape(dataset_images_array.shape[0], -1), stable_hyperdiffusion_images_array.reshape(stable_hyperdiffusion_images_array.shape[0], -1))\n",
    "print(f\"Novelty Hyperdiffusion: {(novelty_hyperdiffusion == -1).sum()}\")\n",
    "print(f\"Novelty Stable Hyperdiffusion: {(novelty_stable_hyperdiffusion == -1).sum()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Calculate inception score (IS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchmetrics.image.inception import InceptionScore\n",
    "import einops\n",
    "\n",
    "is_metric = InceptionScore(normalize=True)\n",
    "hyperdiffusion_images = einops.repeat(hyperdiffusion_images_array, \"b h w -> b c h w\", c=3).to(torch.uint8)\n",
    "scores = []\n",
    "for i in range(len(hyperdiffusion_images)//1000):\n",
    "    scores.append(is_metric(hyperdiffusion_images[i*1000:(i+1)*1000]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"IS Hyperdiffusion: {np.mean(np.array(scores), axis=0)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "stable_hyperdiffusion_images = einops.repeat(stable_hyperdiffusion_images_array, \"b h w -> b c h w\", c=3).to(torch.uint8)\n",
    "scores = []\n",
    "for i in range(len(stable_hyperdiffusion_images) // 1000):\n",
    "    scores.append(is_metric(stable_hyperdiffusion_images[i*1000:(i+1)*1000]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"IS Stable Hyperdiffusion: {np.mean(np.array(scores), axis=0)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Peak Signal-to-Noise Ratio (PSNR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchmetrics\n",
    "\n",
    "psnr = torchmetrics.PeakSignalNoiseRatio()\n",
    "score = psnr(hyperdiffusion_images_array, dataset_images_array)\n",
    "print(f\"PSNR hyperdiffusion: {score}\")\n",
    "score = psnr(stable_hyperdiffusion_images_array, dataset_images_array)\n",
    "print(f\"PSNR stable hyperdiffusion: {score}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Structural Similarity Index (SSIM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_images_array = torch.Tensor(dataset_images)\n",
    "hyperdiffusion_images_array = torch.Tensor(hyperdiffusion_images)[:len(dataset_images)]\n",
    "stable_hyperdiffusion_images_array = torch.Tensor(stable_hyperdiffusion_images)[:len(dataset_images)]\n",
    "print(f\"Dataset images shape: {dataset_images_array.shape}\")\n",
    "print(f\"Hyperdiffusion images shape: {hyperdiffusion_images_array.shape}\")\n",
    "print(f\"Stable hyperdiffusion images shape: {stable_hyperdiffusion_images_array.shape}\")\t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchmetrics\n",
    "\n",
    "ssim = torchmetrics.StructuralSimilarityIndexMeasure(data_range=1.0)\n",
    "hyperdiffusion_images = einops.repeat(hyperdiffusion_images_array, \"b h w -> b c h w\", c=3)\n",
    "stable_hyperdiffusion_images = einops.repeat(stable_hyperdiffusion_images_array, \"b h w -> b c h w\", c=3)\n",
    "dataset_images = einops.repeat(dataset_images_array, \"b h w -> b c h w\", c=3)\n",
    "score = ssim(hyperdiffusion_images, dataset_images)\n",
    "print(f\"SSIM score hyperdiffusion: {score}\")\n",
    "score = ssim(stable_hyperdiffusion_images, dataset_images)\n",
    "print(f\"SSIM score stable hyperdiffusion: {score}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5.2 Weights comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. VAE Latent Space"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sample from latent space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_samples = 50\n",
    "latent_images = model_utils.sample_from_latent_space(\n",
    "    vae, mlp, ref_cp, dataset_tokenized[0][2], config_ae.model.n_tokens, config_ae.model.latent_dim, n_samples\n",
    ")\n",
    "visualization_utils.plot_n_images(latent_images)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Interpolate latent space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_interpolation_steps = 10\n",
    "\n",
    "num_interpolations = 100\n",
    "\n",
    "for i in range(num_interpolations):\n",
    "    idx = random.randint(0, len(dataset_tokenized) - 1)\n",
    "    idy = random.randint(0, len(dataset_tokenized) - 1)\n",
    "    latent_vectors = [dataset_tokenized[idx][0], dataset_tokenized[idy][0]]\n",
    "    latent_positions = [dataset_tokenized[idx][2], dataset_tokenized[idy][2]]\n",
    "    latent_vectors = torch.stack(latent_vectors)\n",
    "    latent_positions = torch.stack(latent_positions)\n",
    "\n",
    "    vae.eval()\n",
    "    with torch.no_grad():\n",
    "        latent_vector, _, _ = vae.encoder(latent_vectors, latent_positions)\n",
    "\n",
    "    latent_images = model_utils.interpolate_latent_space(\n",
    "        latent_vector[0], latent_vector[1], num_interpolation_steps, dataset_tokenized[0][2], vae, ref_cp, mlp\n",
    "    )\n",
    "    print(\"Indices:\", idx, idy)\n",
    "    visualization_utils.plot_n_images(latent_images, row=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "images, recons, mse_weights, mse_images = model_utils.get_n_images_and_mses(vae, dataset_tokenized, mlp, len(dataset_tokenized), device=device, random=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_mse_weight_indices = model_utils.get_best_samples(mse_weights, best_n=10)\n",
    "best_mse_image_indices = model_utils.get_best_samples(mse_images, best_n=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in best_mse_image_indices:\n",
    "    for j in best_mse_image_indices:\n",
    "        latent_vectors = [dataset_tokenized[i][0], dataset_tokenized[j][0]]\n",
    "        latent_positions = [dataset_tokenized[i][2], dataset_tokenized[j][2]]\n",
    "        latent_vectors = torch.stack(latent_vectors)\n",
    "        latent_positions = torch.stack(latent_positions)\n",
    "\n",
    "        vae.eval()\n",
    "        with torch.no_grad():\n",
    "            latent_vector, _, _ = vae.encoder(latent_vectors, latent_positions)\n",
    "\n",
    "        latent_images = model_utils.interpolate_latent_space(\n",
    "            latent_vector[0], latent_vector[1], num_interpolation_steps, dataset_tokenized[0][2], vae, ref_cp, mlp\n",
    "        )\n",
    "        visualization_utils.plot_n_images(latent_images, row=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7. Get nearest neighbors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_samples = 30\n",
    "k = 2\n",
    "dataset_images, hyperdiffusion_images, mse_images, mse_weights, distances = model_utils.generate_nearest_neighbors(hyperdiffusion, \n",
    "                                                                                                                  inr=mlp,\n",
    "                                                                                                                  dataset=dataset_flattened,\n",
    "                                                                                                                  num_samples=30,\n",
    "                                                                                                                  k=k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualization_utils.plot_diffusion_knn(\n",
    "    hyperdiffusion_images,\n",
    "    dataset_images,\n",
    "    mse_images,\n",
    "    mse_weights,\n",
    "    k=k,\n",
    "    num_samples=num_samples\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_images, stable_hyperdifusion_images, mse_images, mse_weights, distances = model_utils.generate_nearest_neighbors(stable_hyperdiffusion, \n",
    "                                                                                                                  inr=mlp,\n",
    "                                                                                                                  dataset=dataset_tokenized,\n",
    "                                                                                                                  num_samples=30,\n",
    "                                                                                                                  k=k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualization_utils.plot_diffusion_knn(\n",
    "    stable_hyperdifusion_images,\n",
    "    dataset_images,\n",
    "    mse_images,\n",
    "    mse_weights,\n",
    "    k=k,\n",
    "    num_samples=num_samples\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analse latent space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_tokens = [dataset_tokenized[i][0] for i in range(len(dataset_tokenized))]\n",
    "dataset_tokens = torch.stack(dataset_tokens)\n",
    "dataset_positions = [dataset_tokenized[i][2] for i in range(len(dataset_tokenized))]\n",
    "dataset_positions = torch.stack(dataset_positions)\n",
    "print(\"Dataset tokens:\", dataset_tokens.shape)\n",
    "print(\"Dataset positions:\", dataset_positions.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"evaluation/stable_hyperdiffusion\"\n",
    "file_paths = [os.path.join(root, file) for root, _, files in os.walk(path) for file in files]\n",
    "print(file_paths)\n",
    "latens = []\n",
    "tokens = []\n",
    "pos = []\n",
    "for file in file_paths:\n",
    "    samples_tokenized, samples_reconstructed, positions = torch.load(file, map_location=\"cpu\")\n",
    "    latens.append(samples_tokenized)\n",
    "    tokens.append(samples_reconstructed)\n",
    "    pos.append(positions)\n",
    "stable_hyperdiffusion_latent = torch.cat(latens, dim=0)\n",
    "stable_hyperdiffusion_recon = torch.cat(tokens, dim=0)\n",
    "stable_hyperdiffusion_pos = torch.cat(pos, dim=0)\n",
    "print(stable_hyperdiffusion_latent.shape)\n",
    "print(stable_hyperdiffusion_recon.shape)\n",
    "print(stable_hyperdiffusion_pos.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "vae.eval()\n",
    "with torch.no_grad():\n",
    "    dataset_latent,_,_ = vae.encoder(dataset_tokens, dataset_positions)\n",
    "print(dataset_latent.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_latent_flattened = dataset_latent.flatten(start_dim =1)\n",
    "stable_hyperdiffusion_latent_flattened = stable_hyperdiffusion_latent.flatten(start_dim =1)\n",
    "print(dataset_latent_flattened.shape)\n",
    "print(stable_hyperdiffusion_latent_flattened.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get cosine similarity for closest latent vectors\n",
    "indices, distances = metrics.find_nearest_neighbor(stable_hyperdiffusion_latent_flattened, dataset_latent_flattened)\n",
    "print(\"Cosine similarity for latent vectors: \", distances.mean().item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get cosine similarity for closest latent vectors\n",
    "num_images = 200\n",
    "distances_total = []\n",
    "for i in range(len(stable_hyperdiffusion_latent)//num_images):\n",
    "    latent_vectors = stable_hyperdiffusion_latent[i*num_images:i*num_images+num_images]\n",
    "    indices, distances = metrics.find_nearest_neighbor(latent_vectors, dataset_latent, metric=\"euclidean\")\n",
    "    distances_total.extend(distances)\n",
    "print(\"Euclidean distance for latent vectors: \", np.mean(distances_total))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get manhattan distance for closest latent vectors\n",
    "num_images = 200\n",
    "distances_total = []\n",
    "for i in range(len(stable_hyperdiffusion_latent)//num_images):\n",
    "    latent_vectors = stable_hyperdiffusion_latent[i*num_images:i*num_images+num_images]\n",
    "    indices, distances = metrics.find_nearest_neighbor(latent_vectors, dataset_latent, metric=\"manhattan\")\n",
    "    distances_total.extend(distances)\n",
    "print(\"Manhattan distance for latent vectors: \", np.mean(distances_total))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 8. Training and inference speed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "stable_hyperdiffusion = stable_hyperdiffusion.to(device)\n",
    "hyperdiffusion = hyperdiffusion.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "%timeit stable_hyperdiffusion.generate_samples(1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "%timeit hyperdiffusion.generate_samples(1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.data.inr_dataset import DataHandler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "config: DiffusionExperimentConfig = DiffusionExperimentConfig.sanity()\n",
    "config.data = DataConfig.small()\n",
    "config.data.batch_size = 128\n",
    "config.data.sample_limit = None\n",
    "print(config.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup dataloaders\n",
    "data_handler = DataHandler(config, use_autoencoder=False)\n",
    "data_handler.setup()\n",
    "hyperdiffusion_loader = data_handler.train_dataloader()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import get_linear_schedule_with_warmup\n",
    "import einops\n",
    "\n",
    "def train(model, data_loader, epochs, device):\n",
    "    optimizer = torch.optim.AdamW(\n",
    "        model.parameters()\n",
    "    )\n",
    "\n",
    "    total_steps = len(data_loader) * epochs\n",
    "    warmup_steps = int(total_steps * 0.1)\n",
    "\n",
    "    # Linear warmup scheduler\n",
    "    scheduler = get_linear_schedule_with_warmup(\n",
    "        optimizer, num_warmup_steps=warmup_steps, num_training_steps=total_steps\n",
    "    )\n",
    "    model = model.to(device)\n",
    "    model.train()\n",
    "    for epoch in range(epochs):\n",
    "        for i, batch in enumerate(data_loader):\n",
    "            if model.autoencoder is None:\n",
    "                \n",
    "                batch = batch.to(device)\n",
    "                loss = model._compute_loss(batch)\n",
    "            else:\n",
    "                original_tokens, _, original_positions = batch\n",
    "                original_tokens = original_tokens.to(device)\n",
    "                original_positions = original_positions.to(device)\n",
    "\n",
    "                with torch.no_grad():\n",
    "                    latent_vector, _, _ = model.autoencoder.encoder(\n",
    "                        original_tokens, original_positions\n",
    "                    )  # Shape: (batch_size, n_tokens, latent_dim)\n",
    "\n",
    "                # Flatten all dimensions after batch (b) into a single dimension (-1 means auto-calculate size)\n",
    "                flattened_latent = latent_vector.flatten(start_dim=1)\n",
    "\n",
    "                loss = model._compute_loss(flattened_latent)\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            scheduler.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "%timeit train(hyperdiffusion, hyperdiffusion_loader, epochs=2, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup dataloaders\n",
    "data_handler = DataHandler(config, use_autoencoder=True)\n",
    "data_handler.setup()\n",
    "stable_hyperdiffusion_loader = data_handler.train_dataloader()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "%timeit train(stable_hyperdiffusion, stable_hyperdiffusion_loader, epochs=2, device=device)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
