data:
  data_path: "lhdm/data/mnist-inrs"
  dataset_type: "MNIST"
  batch_size: 32
  num_workers: 4
  pin_memory: true
  split_ratio: [80, 10, 10]

diffusion:
  timesteps: 1000
  beta_schedule: "linear" # options: linear, cosine
  beta_start: 0.0001
  beta_end: 0.02
  model_mean_type: "EPSILON" # options: EPSILON, START_X, PREVIOUS_X
  model_var_type: "FIXED_SMALL" # options: FIXED_SMALL, FIXED_LARGE, LEARNED
  loss_type: "MSE" # options: MSE, RESCALED_MSE, KL, RESCALED_KL
  use_ddim: true # whether to use DDIM sampling
  ddim_steps: 250 # number of steps for DDIM sampling

model:
  input_dim: 1185 # matches your INR weight dimension
  hidden_dim: 1185
  time_embed_dim: 256
  dropout: 0.1
  n_layers: 3 # number of MLP layers
  activation: "relu"

training:
  lr: 1.0e-4
  weight_decay: 1.0e-5
  epochs: 100
  gradient_clip_val: 1.0
  accumulate_grad_batches: 1
  normalization_factor: 1.0 # scale factor for input data
  eval_interval: 1000 # how often to evaluate and visualize
  save_interval: 5000 # how often to save checkpoints

logging:
  project_name: "inr-diffusion"
  run_name: null # will be auto-generated if not specified
  log_model: false
  save_dir: "logs"
  num_samples_to_visualize: 4
  sample_every_n_epochs: 5
  log_every_n_steps: 100

checkpoint:
  dirpath: "checkpoints"
  filename: "diffusion-{epoch:02d}-{val_loss:.2f}"
  monitor: "val/loss"
  mode: "min"
  save_last: true
  save_top_k: 3

early_stopping:
  monitor: "val/loss"
  min_delta: 1.0e-4
  patience: 50
  mode: "min"
  stopping_threshold: null
  divergence_threshold: null
