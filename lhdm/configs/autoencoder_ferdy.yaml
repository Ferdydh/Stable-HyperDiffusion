data:
  data_path: "lhdm/data/mnist-inrs"
  dataset_type: "MNIST"
  class_label: 2
  # sample_id: 1096
  batch_size: 4 # Increased batch size for better stability
  num_workers: 4
  sample_limit: 4
  split_ratio: [80, 10, 10]

# Autoencoder settings
model:
  max_positions: [100, 10, 40]
  num_layers: 8
  d_model: 1024
  dropout: 0.0
  windowsize: 32
  nhead: 8
  i_dim: 33
  n_tokens: 65
  lat_dim: 128
  odim: 30

training:
  gamma: 0.05
  reduction: "mean"
  temperature: 0.1
  contrast: "simclr"
  z_var_penalty: 0.0

optimizer:
  name: "adamw"
  lr: 1.0e-4 # Slightly lower learning rate for stability
  betas: [0.9, 0.999] # Standard Adam betas
  weight_decay: 3.0e-9
  eps: 1.0e-8

scheduler:
  name: "OneCycleLR"
  #T_max: 1000 # Number of iterations for cosine cycle
  #eta_min: 1.0e-6

trainer:
  max_epochs: 100
  precision: 32
  gradient_clip_val: 1.0
  accumulate_grad_batches: 1
  val_check_interval: 1 # Validate every 0.5 epochs
  log_every_n_steps: 1

logging:
  project_name: "inr-autoencoder"
  run_name: null # Will be auto-generated if not specified
  log_model: false
  save_dir: "logs"
  num_samples_to_visualize: 3 # Number of fixed samples to track
  sample_every_n_epochs: 50 # How often to log sample reconstructions
  log_every_n_steps: 1

checkpoint:
  dirpath: "checkpoints"
  filename: "autoencoder-{epoch:02d}-{val_loss:.2f}"
  monitor: "val/loss"
  mode: "min"
  save_last: true
  save_top_k: 3

early_stopping:
  monitor: "val/loss" # Metric to monitor
  min_delta: 1.0e-5 # Minimum change to qualify as an improvement
  patience: 200 # Number of epochs with no improvement after which training will be stopped
  mode: "min" # "min" for metrics to minimize (like loss), "max" for metrics to maximize
  stopping_threshold: null # Optional: stop training if we reach this value
  divergence_threshold: null # Optional: stop training if we exceed this value
